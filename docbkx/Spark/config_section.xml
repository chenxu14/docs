<?xml version="1.0" encoding="UTF-8"?>
<section version="5.0" xmlns="http://docbook.org/ns/docbook"
	xmlns:xlink="http://www.w3.org/1999/xlink"
	xmlns:xi="http://www.w3.org/2001/XInclude"
	xmlns:ns5="http://www.w3.org/2000/svg"
	xmlns:ns4="http://www.w3.org/1998/Math/MathML"
	xmlns:ns3="http://www.w3.org/1999/xhtml"
	xmlns:db="http://docbook.org/ns/docbook">
	<title>基于YARN的配置明细</title>
	<section>
		<title>Cluster模式</title>
		<orderedlist>
			<listitem>
				<para>spark-default.conf文件中</para>
				<itemizedlist make='bullet'>
					<listitem>
						<para>spark.yarn.applicationMaster.waitTries</para>
						<para>等待指定次数后(默认为10次)，如果SparkContext依然没有成功初始化则放弃等待操作。</para>
					</listitem>
					<listitem>
						<para>spark.yarn.submit.file.replication</para>
						<para>App依赖资源上传到HDFS后的副本备份数量。</para>
					</listitem>
					<listitem>
						<para>spark.yarn.preserve.staging.files</para>
						<para>App运行结束后是否保存staging目录。</para>
					</listitem>
					<listitem>
						<para>spark.yarn.scheduler.heartbeat.interval-ms</para>
						<para>AM与RM心跳时间间隔，默认为5秒。</para>
					</listitem>
					<listitem>
						<para>spark.yarn.max.worker.failures</para>
						<para>当运行出错的worker达到指定数量后，将App标记为failure。</para>
					</listitem>
					<listitem>
						<para>spark.yarn.report.interval</para>
						<para>Client汇报App作业进度的时间频率(毫秒)，默认情况下每隔一秒汇报一次(前提：LOG_LEVEL为INFO)。</para>
					</listitem>
					<listitem>
						<para>spark.yarn.historyServer.address</para>
						<para>Spark的historyServer组件地址，通过它可查看历史作业信息。</para>
					</listitem>
					<listitem>
						<para>spark.driver.extraClassPath</para>
						<para>为AMContainer的运行引入额外的CLASSPATH信息。</para>
					</listitem>
					<listitem>
						<para>spark.executor.extraClassPath</para>
						<para>为ExecutorContainer的运行引入额外的CLASSPATH信息。</para>
					</listitem>
					<listitem>
						<para>spark.driver.extraJavaOptions</para>
						<para>设置AMContainer的jvm启动参数。</para>
					</listitem>
					<listitem>
						<para>spark.executor.extraJavaOptions</para>
						<para>设置ExecutorContainer的jvm启动参数。</para>
					</listitem>
					<listitem>
						<para>spark.driver.extraLibraryPath</para>
						<para>路径指向driver运行所依赖的native类库。</para>
					</listitem>
					<listitem>
						<para>spark.executor.extraLibraryPath</para>
						<para>路径指向executor运行所依赖的native类库。</para>
					</listitem>
					<listitem>
						<para>spark.streaming.kafka.maxRetries</para>
						<para>获取kafka分区偏移量失败后的重试次数(默认只重试一次)，失败重试的等待时间通过refresh.leader.backoff.ms参数来指定(在kafka的消费端进行设置，默认为200毫秒，可提升一个数量级)，相关jira可参考SPARK-7827。</para>
					</listitem>
					<listitem>
						<para>spark.sql.hive.metastorePartitionPruning</para>
						<para>是否启用分区修剪功能，如果不启用spark将会把目标表格的所有分区加载到内存里(Driver端)，以便于加快之后的查询效率。如果表格的分区数较多关闭该功能将有可能产生PacketTooBigException异常，异常信息大致如下：</para>
						<para>com.mysql.jdbc.PacketTooBigException: Packet for query is too large (1315699 > 1048576). You can change this value on the server by setting the max_allowed_packet' variable.</para>
						<para>该参数是从spark1.5版本开始引入的，具体可参考SPARK-6910。</para>
					</listitem>
				</itemizedlist>
			</listitem>
			<listitem>
				<para>spark-env.sh文件中</para>
				<itemizedlist make='bullet'>
					<listitem><para>JAVA_HOME：Java主目录；</para></listitem>
					<listitem><para>HADOOP_CONF_DIR：hadoop config目录；</para></listitem>
					<listitem>
						<para>SPARK_DIST_CLASSPATH：spark运行依赖的库文件；</para>
						<para>如果在编译spark的时指定了-Phadoop-provided配置，则在运行spark的时候需要通过该变量来引入hadoop的相关类库，否则提交作业时会出现NoClassDefFoundError异常。</para>
						<para>export SPARK_DIST_CLASSPATH=$(hadoop classpath)</para>
					</listitem>
					<listitem>
						<para>SPARK_JAR：spark-assembly*.jar存储路径，可存储在本地或hdfs上，如果指向hdfs存储路径则在部署Spark应用时便无需将该jar包进行上传。</para>
						<para>在1.5以后的版本中已不推荐使用，改用spark.yarn.jar配置参数来进行设置。</para>
					</listitem>
					<listitem><para>SPARK_SUBMIT_LIBRARY_PATH：为spark启动环境引入native库。</para></listitem>
					<listitem><para>SPARK_CLASSPATH：为spark启动环境设置classpath信息。</para></listitem>
				</itemizedlist>
			</listitem>
			<listitem>
				<para>yarn-site.xml文件中</para>
				<para>如果Spark是基于YARN环境进行部署的，那么它所采用的hadoop配置是由客户端来决定的，这点与MapReduce不同，每一个map或reduce任务所采用的hadoop配置是通过聚合客户端和NodeManager端的配置而形成的。这样即使相关配置在客户端没有声明，但只要在NodeManager端声明了任务一样可以获取到，而Spark则不同，由于其配置项只由客户端来决定，因此所有其依赖的配置项都需要在客户端进行声明，容易忽略的配置项比如yarn.resourcemanager.webapp.address.&lt;RM_ID>，如果缺少该配置，sparkUI将无法被正常访问，具体原因可参考YARN框架中WebAppUtils类的getProxyHostsAndPortsForAmFilter方法以及SPARK-5837讨论。</para>
			</listitem>
		</orderedlist>
	</section>
	<section>
		<title>Client模式</title>
		<orderedlist>
			<listitem>
				<para>spark-default.conf配置文件同Cluster模式类似；</para>
			</listitem>
			<listitem>
				<para>spark-env.sh文件中</para>
				<itemizedlist make='bullet'>
					<listitem><para>HADOOP_CONF_DIR：hadoop config目录；</para></listitem>
					<listitem><para>SPARK_EXECUTOR_INSTANCES：向YARN平台申请的TaskContainer数量，默认为2；</para></listitem>
					<listitem><para>SPARK_EXECUTOR_CORES：每个TaskContainer使用的cpu数，默认为1；</para></listitem>
					<listitem><para>SPARK_EXECUTOR_MEMORY：每个TaskContainer的堆内存大小，默认为1G；</para></listitem>
					<listitem><para>SPARK_DRIVER_MEMORY：AMContainer的堆内存大小，默认为512M；</para></listitem>
					<listitem><para>SPARK_YARN_APP_NAME：应用程序名称，默认为spark；</para></listitem>
					<listitem><para>SPARK_YARN_QUEUE：应用程序所提交到的目标队列，默认为default；</para></listitem>
					<listitem><para>SPARK_JAR：spark-assembly*.jar存储路径，可存储在本地或hdfs上，如果指向hdfs存储路径则在部署Spark应用时便无需将该jar包进行上传；</para></listitem>
					<listitem><para>SPARK_YARN_USER_ENV：为Container的运行引入额外的环境变量信息(无论是AMContainer还是TaskContainer)，e.g. SPARK_YARN_USER_ENV="CLASSPATH=/foo/bar"表示将/foo/bar追加到环境变量配置里。</para></listitem>
				</itemizedlist>
				<para>除此之外，Spark还对外声明了SPARK_LIBRARY_PATH和SPARK_CLASSPATH变量用于设置客户端的运行环境，如果Hadoop集群启用了LZO功能，可添加如下配置：</para>
				<itemizedlist make='bullet'>
					<listitem>
						<para>SPARK_SUBMIT_LIBRARY_PATH用于指向hadoop-lzo的动态链接库</para>
						<para>如SPARK_SUBMIT_LIBRARY_PATH=$SPARK_SUBMIT_LIBRARY_PATH:/path/to/your/hadoop-lzo/libs/native</para>
					</listitem>
					<listitem>
						<para>SPARK_CLASSPATH用于指向hadoop-lzo的jar包</para>
						<para>如SPARK_CLASSPATH=$SPARK_CLASSPATH:/path/to/your/hadoop-lzo/java/libs</para>
					</listitem>
				</itemizedlist>
				<para>如果使用spark-sql来执行hive查询，还需将hive的元数据库驱动程序加入到SPARK_CLASSPATH环境变量中。</para>
			</listitem>
		</orderedlist>
	</section>
</section>