<?xml version="1.0" encoding="UTF-8"?>
<section version="5.0" xmlns="http://docbook.org/ns/docbook"
	xmlns:xlink="http://www.w3.org/1999/xlink"
	xmlns:xi="http://www.w3.org/2001/XInclude"
	xmlns:ns5="http://www.w3.org/2000/svg"
	xmlns:ns4="http://www.w3.org/1998/Math/MathML"
	xmlns:ns3="http://www.w3.org/1999/xhtml"
	xmlns:db="http://docbook.org/ns/docbook">
	<title>数据shuffle</title>
	<para>从1.2.0版本起，spark开始提供资源弹性申请功能，可根据作业的负载情况来动态的申请/释放Container计算资源(SPARK-3174)。然而由于在此之前map端的输出数据一直是通过TaskExecutor进行管理的，如果在map任务运行结束之后而shuffle操作尚未完成之前TaskExecutor被回收了，那么便会出现shuffle数据丢失的情况，从而在资源回收的过程中产生问题。因此要想对资源合理的回收使用需要首先解决shuffle数据与TaskExecutor解耦的问题。Spark是通过YARN的辅助服务(AuxServices)来实现这一功能特性的，具体的服务实现类为org.apache.spark.network.yarn.YarnShuffleService。服务启动后，shuffle数据通过NodeManager来进行管理，而TaskExecutor只负责执行具体的任务，不再处理数据shuffle相关的逻辑。</para>
	<section>
		<title>通信机制</title>
		<para>shuffle逻辑主要是reduce端向map端取数据的过程，在取数过程中，用于数据传输的通信管道是基于Netty来构建的，其中TransportServer相当于数据传输的Server端，由YarnShuffleService负责创建(代码逻辑参考其serviceInit方法)，其在初始化过程中会执行如下操作：</para>
		<orderedlist>
			<listitem>
				<para>首先实例化Netty的ServerBootstrap对象，通过它来接受客户端的访问请求。</para>
				<para>所采用的通信管道是通过spark.shuffle.io.mode参数来进行声明的，默认基于NIO的方式进行通信。</para>
			</listitem>
			<listitem>
				<para>执行TransportContext类的initializePipeline方法来初始化Netty的ChannelPipeline。</para>
				<para>Pipeline中包含如下几个ChannelHandler：</para>
				<itemizedlist make='bullet'>
					<listitem>
						<para>org.apache.spark.network.protocol.MessageEncoder</para>
						<para>继承至Netty的MessageToMessageEncoder类，用于对传输的消息执行编码。如果消息类型为ChunkFetchSuccess，则编码过程比较特殊，需要将shffuleBlock数据也一并编码到字节流中去，详细参考FileSegmentManagedBuffer类的convertToNetty方法。</para>
					</listitem>
					<listitem>
						<para>io.netty.handler.codec.LengthFieldBasedFrameDecoder</para>
						<para>该handler主要用于确定消息的frame大小，其中前8个字节为目标消息的长度(long类型)。</para>
					</listitem>
					<listitem>
						<para>org.apache.spark.network.protocol.MessageDecoder</para>
						<para>该类继承至Netty的MessageToMessageDecoder，用于解码通信的另一端所发送过来的消息，并将其转换成Message对象。</para>
					</listitem>
					<listitem>
						<para>io.netty.handler.timeout.IdleStateHandler</para>
						<para>用来处理消息(发送/接受)超时的Handler，默认的超时时间通过spark.shuffle.io.connectionTimeout参数进行设置。超时处理可参考TransportChannelHandler类的userEventTriggered方法，主要是打印输出日志和关闭通信Channel。</para>
					</listitem>
					<listitem>
						<para>org.apache.spark.network.server.TransportChannelHandler</para>
						<para>TransportChannelHandler主要继承至Netty的SimpleChannelInboundHandler对象，是真正处理Shuffle业务逻辑的handler。当有消息到达时首先对目标消息的类型进行判断，然后将不同类型的消息委派给相应的MessageHandler进行处理。</para>
					</listitem>
				</itemizedlist>
			</listitem>
		</orderedlist>
		<para>而TransportClient则起到shuffle客户端的作用，负责从TransportServer端抓取shffuleBlock数据，其构造方法是通过TransportClientFactory类的createClient方法来实现的，在构造过程中主要执行以下操作：</para>
		<orderedlist>
			<listitem>
				<para>首先创建出Netty的Bootstrap对象实例，用于和Server端的ServerBootstrap进行通信。</para>
				<para>Bootstrap在实例化过程中所采用的ChannelPipeline与TransportServer端的设置完全相同。</para>
			</listitem>
			<listitem><para>将创建成功的TransportClient对象加入缓冲池，以便于重复使用，类似于数据库连接池的处理方式。</para></listitem>
		</orderedlist>
		<para>TransportClient对外声明了如下两个方法用来和TransportServer进行通信，分别为：</para>
		<itemizedlist make='bullet'>
			<listitem>
				<para>sendRpc或sendRpcSync</para>
				<para>发送RpcRequest消息到TransportServer端，并注册RpcResponseCallback进行回调处理(回调处理逻辑可参考OneForOneBlockFetcher类的start方法)。</para>
			</listitem>
			<listitem>
				<para>fetchChunk</para>
				<para>发送ChunkFetchRequest消息到Server端，并注册BlockFetchingListener进行回调处理(回调处理逻辑可参考ShuffleBlockFetcherIterator类的sendRequest方法)。</para>
			</listitem>
		</itemizedlist>
	</section>
	<section>
		<title>Executor注册</title>
		<para>如之前所述，为了解决因TaskExecutor回收而产生的shuffle数据丢失问题，需要将每一个TaskExecutor的shuffle状态保存下来，这样即使目标TaskExecutor被回收掉，其之前的shuffle状态也可保留。</para>
		<para>shuffle状态主要包含了以下几个方面的元数据信息(通过ExecutorShuffleInfo类来封装)：</para>
		<para>(1)localDirs - shuffle数据的本地存放路径。</para>
		<para>(2)subDirsPerLocalDir - 每个localDir下会创建多少个子文件夹。</para>
		<para>(3)shuffleManager - 所采用的shuffle策略，在1.5.0版本中默认采用SortShuffleManager，可通过spark.shuffle.manager参数来指定。</para>
		<para>注册逻辑主要是通过调用ExternalShuffleClient类的registerWithShuffleServer方法来实现的(Executer启动之后会调用该方法，代码逻辑可参考BlockManager类的registerWithExternalShuffleServer方法)。方法执行之后，会从缓冲池中获取TransportClient对象(如果没有则创建)，然后执行其sendRpcSync方法来向本地的TransportServer发送RegisterExecutor消息，并等待执行结果的返回直至等待时间超过5秒。</para>
		<para>TransportServer端收到消息以后开始进行如下处理(代码逻辑参考ExternalShuffleBlockResolver类的registerExecutor方法)：首先将目标Executor的ShuffleInfo存储到Map集合里，然后持久化到LevelDB中，以便于NodeManager重启后进行状态恢复。LevelDB的存储位置为$yarn.nodemanager.local-dirs/registeredExecutors.ldb，文件查找逻辑可参考YarnShuffleService类的findRegisteredExecutorFile方法。</para>
	</section>
	<section>
		<title>shuffle数据生成</title>
		<para>shuffle数据主要是通过ShuffleMapTask来负责生成的，Spark作业中一共存在两种类型的任务，分别是ResultTask和ShuffleMapTask。其中ResultTask负责将作业的输出结果返回给Driver端，而ShuffleMapTask负责将任务的输出结果进行分区，以便将不同分区的数据传递到其他任务端进行处理。</para>
		<para>默认的分区逻辑一般都是通过HashPartitioner类来封装的(sortByKey操作比较特殊，其采用RangePartitioner来进行分区)，在无特殊指定的情况下其会将任务的输出结果划分成$spark.default.parallelism个区域。如果spark.default.parallelism参数没有指定，那么默认的分区数量需要参考其目标RDD的分区数来决定(代码参考Partitioner类的defaultPartitioner方法)。除此之外在执行reduceByKey、distinct以及groupByKey等操作时还可通过相应的方法参数来手动设置分区数量。</para>
		<para>ShuffleMapTask在运行过程中会去调用ShuffleManager的getWriter方法来获取输出流对象以便将shuffle数据进行保存。如果采用的是SortShuffleManager(默认采用)，将返回SortShuffleWriter输出流实例，然后执行其write方法将任务输出保存至本地，保存过程大致如下：</para>
		<orderedlist>
			<listitem>
				<para>首先构造SortShuffleFileWriter</para>
				<para>如果任务没有启用map端聚合功能，并且要进行shuffle的数据分区数小于200(spark.shuffle.sort.bypassMergeThreshold参数指定)则采用BypassMergeSortShuffleWriter来进行数据写入，否则采用ExternalSorter。</para>
				<tip>Map端聚合功能在一些RDD的调用函数中会默认启用，如reduceByKey方法(代码可参考PairRDDFunctions的reduceByKey函数)，以此来降低要shuffle的数据总量</tip>
			</listitem>
			<listitem>
				<para>执行SortShuffleFileWriter的insertAll方法将shuffle数据写入内存或磁盘</para>
				<itemizedlist make='bullet'>
					<listitem>
						<para>如果采用的是BypassMergeSortShuffleWriter，将每份分区数据分别写入到不同的文件中进行存储，并且数据写入过程中不会执行merge和sort操作(全部放到reduce端处理)。</para>
						<para>此种方式存在一个问题：如果任务输出的分区数量比较庞大，那么map端有可能会占用大量的文件句柄，从而导致内存溢出。因此最好将spark.shuffle.sort.bypassMergeThreshold参数值设置的低一些。</para>
					</listitem>
					<listitem>
						<para>如果采用的是ExternalSorter，需要分两种情况进行处理。</para>
						<para>(1)如果任务启用了map端聚合功能，执行如下操作。</para>
						<para>遍历每一条任务输出记录，将其保存到AppendOnlyMap集合中，Map的key值为(partitionID, mapOutputKey)二元组，如果目标key在Map中存在，将其value值与新值采用mergeValue函数进行合并；否则添加新的记录到Map集合中(新记录的value值在添加前将采用createCombiner函数进行处理)。</para>
						<tip>mergeValue函数和createCombiner函数一般是在对RDD执行转换操作时声明的，比如在对目标RDD执行reduceByKey(_+_)操作时，mergeValue函数为“_+_”，而createCombiner函数为“(v: V) => v”</tip>
						<para>(2)如果任务没有启用map端聚合功能，执行如下操作</para>
						<para>首先判断作业是否支持map输出序列化处理(即spark.shuffle.sort.serializeMapOutputs参数值为true)，并且所采用的spark.serializer配置支持对象重定向功能(比如org.apache.spark.serializer.KryoSerializer)。</para>
						<para>如果以上判断条件满足，将任务输出保存至PartitionedSerializedPairBuffer缓存中进行存储。PartitionedSerializedPairBuffer的缓存空间可划分成两个区域，其中metaBuffer负责存储每条任务输出记录的元数据信息，记录的存储结构如下：</para>
						<programlisting>
    +-------------+------------+------------+-------------+
    |         keyStart         | keyValLen  | partitionId |
    +-------------+------------+------------+-------------+
						</programlisting>
						<para>而kvBuffer负责存储每条任务输出记录的字节流数据，存储结构采用ChainedBuffer来封装，其内部是分chunk进行存储的，每个chunk的数据结构为ArrayBuffer[Array[Byte]]，存储容量为4MB(通过spark.shuffle.sort.kvChunkSize参数指定)。采用划分trunk的方式进行存储好处是扩容比较容易，已有trunk写满后开启一个新的trunk即可；不好的一点是每个trunk的内存地址并不是连续的。</para>
						<para>无论基于那种情况进行处理，都需要保证在shuffle数据写入内存的时候不会产生OOM异常。为此Spark提供了一套spill机制，当满足以下判断条件时需要将内存中的shuffle数据flush到磁盘上(代码逻辑参考Spillable类的maybeSpill方法)：</para>
						<para>条件1：向内存中写入shuffle数据的记录数达到spark.shuffle.spill.numElementsForceSpillThreshold参数阈值(默认不限制)</para>
						<para>条件2：向内存中写入shuffle数据的总量已经达到一定阈值(详细参考内存管理部分)</para>
						<para>flush过程主要是调用ExternalSorter类的spill方法，大致流程如下：首先在本地创建$app_local_dir/blockmgr_$uid/$int/temp_shuffle_$uid临时文件，然后构造DiskBlockObjectWriter用于向该临时文件写入shuffle数据，写入之前需要首先对目标shuffle数据进行排序处理，排序规则是由shuffle数据在内存中的组织结构来决定的。</para>
						<para>(1)如果是通过AppendOnlyMap来组织的(即启用了map端聚合功能)，排序规则如下：</para>
						<para>如之前所描述，AppendOnlyMap的key值类型为(partitionID, mapOutputKey)二元组，在排序处理中需要首先对partitionID进行排序，如果partitionID相同在对mapOutputKey进行比较。在比较mapOutputKey的过程中首先判断用户是否手动声明了排序规则，如果没有则通过比较两个Key的hashcode来进行排序(代码逻辑可参考WritablePartitionedPairCollection类partitionKeyComparator方法)。</para>
						<para>(2)如果是通过PartitionedSerializedPairBuffer来组织的，那么只需对metaBuffer进行排序即可，因为metaBuffer相当于是所有shuffle数据的索引。排序算法采用TimSort，代码逻辑可参考TimSort类的sort方法。</para>
						<para>排序操作处理完成之后，将排好序的shuffle数据分批次flush到之前创建的临时文件中，每个批次能够flush的KeyValue上限是通过spark.shuffle.spill.batchSize参数来设置的(默认为10000)。</para>
					</listitem>
				</itemizedlist>
			</listitem>
			<listitem>
				<para>将步骤2所产生的shuffle数据持久化到同一个文件中进行存储(通过调用SortShuffleFileWriter类的writePartitionedFile方法)</para>
				<para>所持久化的文件路径为$app_local_dir/blockmgr_$uid/$int/shuffle_$shuffleId_$mapId_0.data，文件的父路径是通过相关的hash函数对文件名进行映射而来(具体参考DiskBlockManager类的getFile方法)。</para>
				<itemizedlist make='bullet'>
					<listitem>
						<para>如果采用的是BypassMergeSortShuffleWriter</para>
						<para>将步骤2中所生成的所有分区文件合并到目标文件中。</para>
					</listitem>
					<listitem>
						<para>如果采用的是ExternalSorter，执行如下操作。</para>
						<para>首先判断本地是否存在spill文件，如果不存在，则将内存中所有的shuffle数据写入到shuffle_$shuffleId_$mapId_0.data文件中。同样，在写入之前需要先进行排序处理，排序规则同spill过程类似。</para>
						<para>否则，将所有的spill文件连同内存中的shuffle数据统一合并到目标文件中去。合并的过程有可能需要执行聚合以及排序处理，比如作业启用了map端聚合功能，或者用户在调用sortByKey方法时手动声明了排序算法(代码逻辑参考ExternalSorter类的merge方法)。</para>
					</listitem>
				</itemizedlist>
			</listitem>
			<listitem>
				<para>为步骤3所生成的shuffle数据文件创建索引(通过调用IndexShuffleBlockResolver类的writeIndexFile方法)</para>
				<para>索引文件的存放位置为$app_local_dir/blockmgr_$uid/$int/shuffle_$shuffleId_$mapId_0.index，每条索引记录对应每个shuffle分区在目标shuffle_$shuffleId_$mapId_0.data文件中的偏移量位置。</para>
			</listitem>
		</orderedlist>
	</section>
	<section>
		<title>shuffle数据获取</title>
		<para>在对ShuffledRDD执行compute方法时会触发Shuffle操作的获取逻辑，方法执行之后，会首先调用ShuffleManager的getReader方法来获取ShuffleReader实例，然后调用目标Reader的read方法进行shuffle数据读取，在无特殊指定的情况下，默认的Reader实现为BlockStoreShuffleReader，其在执行read方法时会首先构造出ShuffleBlockFetcherIterator实例，并通过它来进行数据获取。</para>
		<para>获取之前需要首先知道目标shuffle数据都存放在哪些机器上，这主要是通过访问Driver来实现的，通过调用MapOutputTracker类的getMapSizesByExecutorId方法。方法在执行过程中会通过AKKA来与MapOutputTrackerMasterEndpoint进行通信，向其发送GetMapOutputStatuses申请，以便获取目标shuffle的元数据信息。知道目标shuffle的存储地址后开始进行读取操作，读取过程中进行如下判断：</para>
		<itemizedlist>
			<listitem>
				<para>如果要读取的shuffle数据是存放在本地的，直接调用BlockManager类的getBlockData方法进行处理，处理过程大致如下。</para>
				<para>首先从BlockId方法参数中获取shuffleId、mapId和reduceId等信息，并通过shuffleId和mapId来定位目标shuffle数据的本地存放位置。</para>
				<para>找到数据索引文件之后(索引文件命名规范：shuffle_$shuffleId_$mapId_0.index)，在通过reduceId来定位其在数据文件中的偏移量位置，并从该位置处读取出目标shuffle的分区数据。</para>
			</listitem>
			<listitem>
				<para>如果要读取的shuffle数据是存放在远端的，需要借助于Netty来进行数据读取，整个通信过程大致如下。</para>
				<para>第一阶段：调用TransportClient类的sendRpc方法向目标TransportServer发送OpenBlocks消息。该操作是通过执行ExternalShuffleClient类的fetchBlocks方法来触发的，调用逻辑可参考ShuffleBlockFetcherIterator类的sendRequest方法。</para>
				<para>TransportServer端收到消息以后开始进行如下处理(代码逻辑参考ExternalShuffleBlockHandler类的handleMessage方法)：</para>
				<orderedlist>
					<listitem>
						<para>首先从目标消息中解析出appId和execId信息，通过它来定位目标shuffle数据由哪个Executor生成。</para>
						<para>如之前所描述(Executor注册章节)，Executor在启动之后会将自己注册到YarnShuffleService服务中，因此这里可通过appId和execId来进行查找。</para>
					</listitem>
					<listitem>
						<para>然后从目标消息中解析出客户端要获取的blockId集合。</para>
						<para>每个blockId的命名规范如下：shuffle_${ShuffleId}_${MapId}_${ReduceId}</para>
					</listitem>
					<listitem>
						<para>获取每一个blockId对应的shuffle分区数据，并将其封装成ManagedBuffer对象(代码逻辑参考ExternalShuffleBlockResolver类的getBlockData方法)。</para>
						<para>获取逻辑由目标Executor所采用的ShuffleManager类型来决定，默认采用的SortShuffleManager，处理逻辑如下：</para>
						<para>首先由shuffleId和mapId定位到目标shuffle数据的索引文件，然后查找索引文件，并通过reduceId来定位目标shuffle数据在data文件中的偏移量位置，最后从data文件的指定偏移量处读取出目标shuffle数据，并将其封装成FileSegmentManagedBuffer对象来返回。</para>
					</listitem>
					<listitem>
						<para>将步骤3所生成的ManagedBuffer集合封装成一个Stream，并注册到streams集合中去。</para>
						<para>shuffle数据的获取是分两阶段来完成的，第一阶段主要是为了开启目标shuffle数据在Server端的输入流，第二阶段开始对目标输入流数据进行读取。</para>
					</listitem>
					<listitem>
						<para>最后发送StreamHandle响应到TransportClient端，向其通知目标shuffle数据的输入流已成功构建。</para>
						<para>TransportClient接受到消息以后，开始调用TransportRequestHandler的processRpcRequest方法进行第二阶段的处理。</para>
					</listitem>
				</orderedlist>
				<para>第二阶段：发送ChunkFetchRequest请求到TransportServer端，以便进行shuffle数据读取。该操作是通过执行TransportClient类的fetchChunk方法来触发的，调用逻辑可参考OneForOneBlockFetcher类的start方法。</para>
				<para>TransportServer端收到消息以后开始调用TransportRequestHandler类的processFetchRequest方法进行处理，处理逻辑如下：</para>
				<orderedlist>
					<listitem>
						<para>将当前的Netty通信管道与第一阶段所创建的Stream进行绑定，以便于将shuffle数据通过该管道发送至客户端(代码逻辑参考StreamManager类的registerChannel方法)。</para>
					</listitem>
					<listitem>
						<para>读取目标Stream中的shuffle数据，通过StreamManager类的getChunk方法。</para>
						<para>如第一阶段所描述，从Stream中读取出的shuffle数据是通过FileSegmentManagedBuffer对象进行封装的，将其返回给TransportClient之前需要首先对其进行编码操作，编码过程主要是调用了FileSegmentManagedBuffer类的convertToNetty方法，通过它来将ManagedBuffer转换成Netty的FileRegion对象。</para>
					</listitem>
					<listitem>
						<para>最后发送ChunkFetchSuccess响应到TransportClient端，连同客户端要获取的shuffle数据。</para>
					</listitem>
				</orderedlist>
			</listitem>
		</itemizedlist>
		<para>shuffle数据是一边读取一边消费的过程，如果数据的读取速度大于消费速度那么有可能造成reduce端内存溢出，因此需要做限速处理。限速逻辑同样是通过ShuffleBlockFetcherIterator类来封装的，针对每个目标机器上所有要获取的shuffle数据，其会构造多个FetchRequest申请，每个申请所返回的shuffle数据总量不能大于spark.reducer.maxSizeInFlight/5参数阈值(代码逻辑参考其splitLocalRemoteBlocks方法)。</para>
		<para>当目标FetchRequest集合构建成功以后，会首先打乱其排序顺序，然后依次遍历每一个FetchRequest，通过sendRequest方法将其发送到TransportServer端进行处理。在请求发送之前，线程会统计所有已请求获取的总数据量大小，如果其值满足以下判断条件：</para>
		<blockquote><para>已请求获取的总数据量 - 已被消费的数据量 > spark.reducer.maxSizeInFlight</para></blockquote>
		<para>将停止向Server端继续发送申请，直至部分已获取的shuffle数据被消费掉为止。</para>
	</section>
	<section>
		<title>相关配置</title>
		<itemizedlist make='bullet'>
			<listitem>
				<para>spark.shuffle.io.mode</para>
				<para>采用哪种类型的通信管道，可选类型有NIO(默认)和EPOLL(只有Linux平台可用)。</para>
			</listitem>
			<listitem>
				<para>spark.shuffle.io.preferDirectBufs</para>
				<para>优先使用直接内存来缓存Netty通信数据(堆外)，默认值为true。</para>
			</listitem>
			<listitem>
				<para>spark.shuffle.io.connectionTimeout</para>
				<para>Netty通信超时时间，默认为120秒。</para>
			</listitem>
			<listitem>
				<para>spark.shuffle.io.backLog</para>
				<para>请求队列的最大长度，默认为-1，表示使用Netty框架默认的设置。</para>
			</listitem>
			<listitem>
				<para>spark.shuffle.io.serverThreads &amp; spark.shuffle.io.clientThreads</para>
				<para>Netty Server端和Client端的处理线程数，默认为2倍的系统processor数(如果其值为0)。</para>
			</listitem>
			<listitem>
				<para>spark.shuffle.io.receiveBuffer &amp; spark.shuffle.io.sendBuffer</para>
				<para>最优值通过如下公式计算：latency * network_bandwidth</para>
				<para>假设latency为1毫秒，带宽为10Gb/s，则buffer为(10000000000 / 8) * 0.001 = 1.25M</para>
				<para>默认值为-1，表示使用Netty框架默认的设置。</para>
			</listitem>
			<listitem>
				<para>spark.shuffle.io.maxRetries</para>
				<para>fetchBlock操作的出错重试次数，默认重试3次。</para>
			</listitem>
			<listitem>
				<para>spark.shuffle.manager</para>
				<para>采用的ShuffleManager实现，目前有3种实现，分别为sort(默认), hash和tungsten-sort。</para>
			</listitem>
			<listitem>
				<para>spark.shuffle.service.enabled</para>
				<para>启用YarnShuffleService服务，通过NodeManager来管理shuffle数据的状态。</para>
			</listitem>
			<listitem>
				<para>spark.shuffle.service.port</para>
				<para>Shuffle操作Server端的端口号，需要在yarn-site.xml中配置，默认为7337。</para>
			</listitem>
			<listitem>
				<para>spark.reducer.maxSizeInFlight</para>
				<para>通过该参数来限制shuffle数据的获取速度，详细参考shuffle数据获取章节，默认值为48mb。</para>
			</listitem>
			<listitem>
				<para>spark.shuffle.reduceLocality.enabled</para>
				<para>为reduce任务分配槽位的时候，是否要最大程度的满足shuffle数据本地获取。默认值为true，这样可能会存在一些问题，参考详细SPARK-10087。</para>
			</listitem>
			<listitem>
				<para>spark.shuffle.sort.bypassMergeThreshold</para>
				<para>如果shuffle数据的分区数小于该参数值，将每个分区数据分别写入到单独的数据文件中进行存储(最后在将这些文件合并成一个)，写入过程中不会执行sort和merge操作，全部放到reduce端去完成，参数默认值为200。</para>
			</listitem>
			<listitem>
				<para>spark.shuffle.sort.serializeMapOutputs</para>
				<para>是否对shuffle数据进行序列化，默认值为true。</para>
			</listitem>
			<listitem>
				<para>spark.shuffle.spill.numElementsForceSpillThreshold</para>
				<para>向内存中写入的shuffle记录数达到该参数值时，需要执行spill操作，默认不限制。</para>
			</listitem>
			<listitem>
				<para>spark.shuffle.io.numConnectionsPerPeer</para>
				<para>该参数用于指定shuffle过程中，针对每个目标TransportServer需要构建多少个TransportClient，这些TransportClient将会被重复使用以达到共享链接的目的，默认只创建一个链接，如果作业的Executor只是部署在少量的NodeManager上，可以适当的提高该参数值来加快shuffle的获取效率。</para>
			</listitem>
		</itemizedlist>
	</section>
</section>