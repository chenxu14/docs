<?xml version="1.0" encoding="UTF-8"?>
<section version="5.0" xmlns="http://docbook.org/ns/docbook"
	xmlns:xlink="http://www.w3.org/1999/xlink"
	xmlns:xi="http://www.w3.org/2001/XInclude"
	xmlns:ns5="http://www.w3.org/2000/svg"
	xmlns:ns4="http://www.w3.org/1998/Math/MathML"
	xmlns:ns3="http://www.w3.org/1999/xhtml"
	xmlns:db="http://docbook.org/ns/docbook">
	<title>流计算应用</title>
	<section>
		<title>Kafka数据</title>
		<para>针对kafka数据源，spark对外声明了两种实现方式来对目标数据源进行消费处理，分别基于Receiver模式和Direct模式，其中Direct模式是从1.3版本起开始引入的。
		</para>
		<section>
			<title>Receiver模式</title>
			<orderedlist>
				<listitem>
					<para>数据读取</para>
					<para>数据读取操作是采用kafka的High-Level-API来实现的，多个Reciver构成一个ConsumerGroup来对目标kafka集群中的Topic进行持续的消费处理。每次从kafka集群读取数据时，首先将数据内容写入WAL日志，以便后续进行容灾恢复处理；然后将已读取的数据偏移量信息写入Zookeeper，这样在执行下次读取时，便可跳过之前已读取的数据，从指定位置处进行消费即可。偏移量信息在Zookeeper中的存储路径如下：
					</para>
					<blockquote>
						<para>/consumers/[ConsumerGroup]/offsets/[topic]/[partition]
						</para>
					</blockquote>
				</listitem>
				<listitem>
					<para>任务加载</para>
					<para>Reciver线程启动后，会从kafka集群持续接收数据，并且每隔一段时间生成一个Block(时间阀值是通过spark.streaming.blockInterval参数来指定的)，Block生成后将其载入内存，并写入WAL以便后续进行容灾恢复处理(如果启用了spark.streaming.receiver.writeAheadLog.enable配置)。</para>
					<para>而在driver端，构造StreamingContext对象时会默认开启RecurringTimer定时器，该定时器会每隔固定时间向JobGenerator提交GenerateJobs事件，事件触发后，driver进程首先通过ReceiverTracker组件来获取每一个Receiver所接受到的Block数据，并将这些Block的元数据信息写入WAL，元数据包括：</para>
					<blockquote>
						<para>(1)块的引用ID：用于定位其在目标Executor中的存储位置；</para>
						<para>(2)数据块在WAL中的偏移量位置。</para>
					</blockquote>
					<para>获取到这些Block数据之后，driver端开始构造BlockRDD数据集(将每个Block作为RDD的分区)，并通过SparkContext的runJob方法将目标RDD提交到Spark运行环境中去运行处理。Spark会针对每个Block开启Task任务，并分发到Executor端去运行。</para>
					<tip>需要注意的是为了确保Task能够在Executor端运行，需要为Executor分配足够的cpu数，如果每个Executor只分配了一个cpu，并且其上部署了Reviber线程，那么便没有其他的槽位来部署Task线程。</tip>
					<para>如果启用了checkpoint功能，默认情况下driver还会每隔batchInterval执行一次checkpoint操作，用于保存当前的作业信息，整个工作流程如图所示：</para>
					<mediaobject>
						<imageobject>
							<imagedata contentdepth="100%" width="100%" scalefit="1"
								fileref="../media/spark/receiver-launch.png"></imagedata>
						</imageobject>
					</mediaobject>
				</listitem>
				<listitem>
					<para>作业容灾</para>
					<para>Driver进程宕机重启后，会首先读取checkpoint信息以便重新构造出StreamingContext对象。然后读取WAL中记录的数据块元数据信息，根据这些数据块重新构造出作业实例。
					</para>
					<para>作业在运行过程中会被拆分成多个Task，每个Task从WAL中读取与之对应的Block数据，并执行相关的计算处理。
					</para>
					<para>最后，Reciver从指定偏移量处继续读取kafka数据进行消费处理(偏移量信息保存在Zookeeper节点上)。整个恢复流程如图所示：
					</para>
					<mediaobject>
						<imageobject>
							<imagedata contentdepth="100%" width="100%" scalefit="1"
								fileref="../media/spark/receiver-recover.jpg"></imagedata>
						</imageobject>
					</mediaobject>
				</listitem>
			</orderedlist>
		</section>
		<section>
			<title>Direct模式</title>
			<para>在启用WAL的前提下(spark.streaming.receiver.writeAheadLog.enable)，虽然Reciver模式可以实现作业容灾功能，但却难以保证每条记录的消费过程是Exactly-once(只被消费一次)。
			</para>
			<mediaobject>
				<imageobject>
					<imagedata contentdepth="45%" width="45%"
						fileref="../media/spark/reciver-error.png"></imagedata>
				</imageobject>
			</mediaobject>
			<para>如上图所示，当Receiver接受到输入流数据并成功写入WAL之后，却没能将目标数据的偏移量信息写入Zookeeper。这时便会产生以下问题：
			</para>
			<para>由于kafka集群没有收到Reciver端的反馈，其会把之前发送的数据当成消费失败来处理，然后将这部分数据内容重新发送到Receiver端，这样便产生了数据重复消费的问题。因此从某种意义上说Reciver模式只是实现了At-least-once，而不是Exactly-once。
			</para>
			<para>为了修复该问题，Spark在1.3版本中引入了另外一种消费模式，即Direct模式，具体工作流程如图所示：</para>
			<mediaobject>
				<imageobject>
					<imagedata contentdepth="65%" width="65%"
						fileref="../media/spark/direct-flow.png"></imagedata>
				</imageobject>
			</mediaobject>
			<orderedlist>
				<listitem>
					<para>任务加载</para>
					<para>首先对kafka集群的目标topic进行读取，获取其内部每一个分区的lastOffset信息，在根据最近一次成功消费的分区偏移量来计算出此次消费待处理的数据区间集合。</para>
					<para>然后针对这些数据区间构建KafkaRDD，把每一个数据区间当成是RDD的分区来处理，对其构造Task进行消费。由此看来KafkaRDD的分区结构与目标topic的分区是一一对应的，形成end-to-end消费模式。</para>
				</listitem>
				<listitem>
					<para>数据读取</para>
					<para>Task任务启动后，将采用SimpleApi直接从kafka的目标分区上进行数据读取，读取指定长度的数据区间之后在做计算处理。由此看来Direct模式与Reciver模式主要区别之一便是：数据读取操作是发生在作业构建之后而不是作业构建之前。</para>
				</listitem>
				<listitem>
					<para>作业容灾</para>
					<para>采用Direct模式之后无需在启用WAL功能，只需将目标分区的偏移量信息写入checkpoint即可，Driver进程重启后会根据这些偏移量信息来重新构建Task任务，并从kafka集群中再次读取指定区间上的数据内容。</para>
				</listitem>
			</orderedlist>
			<para>基于Direct模式做流式计算处理大致可拆分成如下几个步骤：</para>
			<blockquote><para>数据读取 --> 计算分析 --> 计算结果输出 --> 偏移量保存</para></blockquote>
			<para>需要注意的是Direct模式只能保证数据读取与偏移量保存是在同一个事务中，但是并不能保证计算结果的输出也在相同的事务里，因此针对同一条数据记录可能会产生多个输出结果，比如在执行偏移量保存的时候发生了异常，为此cloudera社区对外声明了两种解决办法(详细可参考http://blog.cloudera.com/blog/2015/03/exactly-once-spark-streaming-from-apache-kafka)：</para>
			<orderedlist>
				<listitem>
					<para>采用幂等的方式进行数据写入</para>
					<para>即在计算结果输出之前为其指定如下格式的key值：topic/partition/offset</para>
					<para>这样，即使目标数据集(从offset起始)被消费多次，其输出结果也只有一个，因为它们的key值相同。</para>
				</listitem>
				<listitem>
					<para>将计算结果的输出与偏移量位置的保存放到同一事务里</para>
					<para>偏移量信息默认是保存在checkpoint里的，为了避免采用分布式事务，需要将其数据内容与每次计算的输出结果保存到同一存储媒介中(该存储媒介需要支持事务，比如mysql)。经过这样处理之后，checkpoint中的偏移量信息将失去价值，所有容灾处理将同一采用数据库中的记录信息。</para>
				</listitem>
			</orderedlist>
		</section>
		<section>
			<title>Streaming启动过程</title>
			<para>SparkStreaming的启动时序如下图所示：</para>
			<mediaobject>
				<imageobject>
					<imagedata contentdepth="100%" width="100%" scalefit="1" fileref="../media/spark/sequence.jpg"></imagedata>
				</imageobject>
			</mediaobject>
			<para>StreamingContext作为整个流计算的切入点主要封装了一些上下文环境，包括：SparkContext对象、checkpoint信息以及整个流计算的执行频率。</para>
			<para>JobScheduler作为其内部核心的工作组件，主要用来处理与作业调度相关的逻辑，而在作业生成方面主要通过调用JobGenerator类来完成的，</para>
			<para>JobGenerator顾名思义，主要用来处理与作业生成相关的逻辑，其内部封装了两大组件：RecurringTimer和eventActor。其中RecurringTimer启到定时器的作用，会周期性的向eventActor触发GenerateJobs事件，事件触发以后eventActor会进行相应的回调处理。在处理过程中主要是调用DStream的generateJob方法来生成目标作业。</para>
			<para>DStream在执行generateJob方法时，会首先调用compute方法来生成目标RDD数据集，随后通过SparkContext的runJob方法将目标RDD提交到spark运行环境中去处理。Receiver模式与Direct模式最大的区别便体现在compute方法的计算处理上。</para>
			<para>如果采用Receiver模式，其DStream的实现类ReceiverInputDStream，在执行compute方法时首先调用ReceiverTracker类，通过其getBlocksOfBatch方法来获取所有Reciver所接受到的Block信息，然后基于这些Block来构建BlockRDD数据集，将每一个Block当成是RDD的一个分区进行处理。</para>
			<para>而如果采用Direct模式，其DStream的实现类DirectKafkaInputDStream，在执行compute方法时会首先获取目标kafka集群指定Topic的所有偏移量信息，然后将其与本地的偏移量信息进行比较，从而生成待处理数据区间集合。最后基于这些数据区间结合构建KafkaRDD，将每一个数据区间当成是RDD的一个分区进行处理。</para>
		</section>
	</section>
</section>