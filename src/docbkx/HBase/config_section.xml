<?xml version="1.0" encoding="UTF-8"?>
<section version="5.0" xmlns="http://docbook.org/ns/docbook"
	xmlns:xlink="http://www.w3.org/1999/xlink"
	xmlns:xi="http://www.w3.org/2001/XInclude"
	xmlns:ns5="http://www.w3.org/2000/svg"
	xmlns:ns4="http://www.w3.org/1998/Math/MathML"
	xmlns:ns3="http://www.w3.org/1999/xhtml"
	xmlns:db="http://docbook.org/ns/docbook">
	<title>常用配置</title>
	<para>与hbase相关的常用配置如下：</para>
	<section>
		<title>hbase-site.xml</title>
		<itemizedlist>
			<listitem>
				<para>hbase.cluster.distributed</para>
				<para>是否基于HDFS部署(设置成true)。</para>
			</listitem>
			<listitem>
				<para>zookeeper.znode.parent</para>
				<para>ZK根目录地址(设置成/hbase)。</para>
			</listitem>
			<listitem>
				<para>hbase.regionserver.global.memstore.size.lower.limit</para>
				<para>memstore总使用量达到该阈值开始触发flush(设置成0.9)。</para>
			</listitem>
			<listitem>
				<para>hbase.hregion.memstore.flush.size</para>
				<para>单memstore使用达到该阈值开始进行flush(设置成134217728)</para>
			</listitem>
			<listitem>
				<para>hbase.hregion.max.filesize</para>
				<para>Region的拆分阈值(设置成53687091200)</para>
			</listitem>
			<listitem>
				<para>zookeeper.session.timeout</para>
				<para>ZK的session超时时间(设置成60000)</para>
			</listitem>
			<listitem>
				<para>hbase.regionserver.codecs</para>
				<para>所支持的压缩算法(设置成lzo)</para>
			</listitem>
			<listitem>
				<para>hbase.storescanner.parallel.seek.enable</para>
				<para>是否开启并发seek功能(设置成false)</para>
			</listitem>
			<listitem>
				<para>hbase.snapshot.enabled</para>
				<para>是否开启快照功能(设置成true)</para>
			</listitem>
			<listitem>
				<para>hbase.quota.enabled</para>
				<para>是否开启限额功能(设置成false)</para>
			</listitem>
			<listitem>
				<para>hbase.ipc.server.callqueue.handler.factor</para>
				<para>队列占handler总数的百分比(设置成0.1)</para>
			</listitem>
			<listitem>
				<para>hbase.assignment.usezk</para>
				<para>是否基于ZK做region分配(设置成false)</para>
			</listitem>
			<listitem>
				<para>hbase.storescanner.parallel.seek.threads</para>
				<para>并发seek的线程数(设置成20)</para>
			</listitem>
			<listitem>
				<para>hbase.regionserver.executor.openregion.threads</para>
				<para>处理region加载操作的线程数(设置成20)</para>
			</listitem>
			<listitem>
				<para>hbase.regionserver.executor.closeregion.threads</para>
				<para>处理region关闭操作的线程数(设置成20)</para>
			</listitem>
			<listitem>
				<para>hbase.regionserver.handler.count</para>
				<para>处理客户端RPC请求的线程数(设置成80)</para>
			</listitem>
			<listitem>
				<para>hbase.hstore.flusher.count</para>
				<para>处理flush操作的线程数(设置成20)</para>
			</listitem>
			<listitem>
				<para>hbase.regionserver.thread.compaction.large</para>
				<para>处理大整理操作的线程数(设置成5)</para>
			</listitem>
			<listitem>
				<para>hbase.regionserver.thread.compaction.small</para>
				<para>处理小整理操作的线程数(设置成15)</para>
			</listitem>
			<listitem>
				<para>hbase.regionserver.hlog.syncer.count</para>
				<para>处理wal同步操作线程数(设置成5)</para>
			</listitem>
			<listitem>
				<para>hbase.bucketcache.combinedcache.enabled</para>
				<para>是否LruBlockCache存索引块，BucketCache存数据块(设置成true)</para>
			</listitem>
			<listitem>
				<para>hbase.bucketcache.writer.threads</para>
				<para>写block到BucketCache的线程数(设置成10)</para>
			</listitem>
			<listitem>
				<para>hbase.bucketcache.writer.queuelength</para>
				<para>每个写队列的上线limit(设置成128)</para>
			</listitem>
			<listitem>
				<para>hbase.wal.provider</para>
				<para>WAL策略(设置成multiwal)</para>
			</listitem>
			<listitem>
				<para>hbase.wal.regiongrouping.numgroups</para>
				<para>并发开启多个WAL进行写入(设置成4)</para>
			</listitem>
			<listitem>
				<para>hbase.regionserver.wal.enablecompression</para>
				<para>WAL是否启用压缩(设置成false)</para>
			</listitem>
			<listitem>
				<para>hbase.regionserver.hlog.blocksize</para>
				<para>WAL数据块大小(设置成268435456)</para>
			</listitem>
			<listitem>
				<para>hbase.regionserver.hlog.replication</para>
				<para>WAL块副本数(设置成2)</para>
			</listitem>
			<listitem>
				<para>hbase.fs.async.create.retries</para>
				<para>创建WAL失败重试次数(设置成3)</para>
			</listitem>
			<listitem>
				<para>hbase.lease.recovery.timeout</para>
				<para>等待WAL租约恢复时间(设置成3500)</para>
			</listitem>
			<listitem>
				<para>hbase.regionserver.flush.policy</para>
				<para>flush策略(设置成org.apache.hadoop.hbase.regionserver.FlushAllLargeStoresPolicy)</para>
			</listitem>
			<listitem>
				<para>hbase.regionserver.region.split.policy</para>
				<para>Region拆分策略(设置成org.apache.hadoop.hbase.regionserver.ConstantSizeRegionSplitPolicy)</para>
			</listitem>
			<listitem>
				<para>hbase.hregion.memstore.chunkpool.maxsize</para>
				<para>chunkpool最大使用空间占memstore的百分比(设置成1)</para>
			</listitem>
			<listitem>
				<para>hbase.hregion.memstore.chunkpool.initialsize</para>
				<para>chunkpool初始空间占用百分比(设置成1)</para>
			</listitem>
			<listitem>
				<para>hbase.hregion.majorcompaction</para>
				<para>大整理触发周期(设置成0)</para>
			</listitem>
			<listitem>
				<para>hbase.hstore.compaction.max.size</para>
				<para>每次参与整理操作的最大数据量(设置成21474836480)</para>
			</listitem>
			<listitem>
				<para>hbase.hstore.engine.class</para>
				<para>采用的整理引擎(设置成org.apache.hadoop.hbase.regionserver.StripeStoreEngine)</para>
			</listitem>
			<listitem>
				<para>hbase.store.stripe.compaction.flushToL0</para>
				<para>是否向L0区域写入HFile(设置成true)</para>
			</listitem>
			<listitem>
				<para>hbase.store.stripe.compaction.minFilesL0</para>
				<para>L0文件数达到多少时触发整理(设置成4)</para>
			</listitem>
			<listitem>
				<para>hbase.store.stripe.compaction.minFiles</para>
				<para>stripe文件数达到多少时触发整理(设置成4)</para>
			</listitem>
			<listitem>
				<para>hbase.store.stripe.compaction.maxFiles</para>
				<para>stripe整理能包含的最大文件数(设置成24)</para>
			</listitem>
			<listitem>
				<para>hbase.hstore.blockingStoreFiles</para>
				<para>store下面为文件数达到该阈值时阻塞flush(设置成300)</para>
			</listitem>
			<listitem>
				<para>hbase.store.stripe.initialStripeCount</para>
				<para>stripe的初始数量(设置成2)</para>
			</listitem>
			<listitem>
				<para>hbase.store.stripe.sizeToSplit</para>
				<para>stripe的拆分阈值(设置成10737418240)</para>
			</listitem>
			<listitem>
				<para>hbase.regionserver.throughput.controller</para>
				<para>流控策略类(设置成org.apache.hadoop.hbase.regionserver.compactions.PressureAwareCompactionThroughputController)</para>
			</listitem>
			<listitem>
				<para>hbase.hstore.compaction.throughput.higher.bound</para>
				<para>流量带宽使用上限(设置成4242880000)</para>
			</listitem>
			<listitem>
				<para>hbase.hstore.compaction.throughput.lower.bound</para>
				<para>流量带宽使用下限(设置成1060720000)</para>
			</listitem>
			<listitem>
				<para>hbase.hstore.compaction.ratio.offpeak</para>
				<para>非尖峰时段整理ratio值(设置成5)</para>
			</listitem>
			<listitem>
				<para>hfile.format.version</para>
				<para>HFile版本数(设置成3)</para>
			</listitem>
			<listitem>
				<para>hbase.mob.file.cache.size</para>
				<para>缓存的mob文件具柄数(设置成500)</para>
			</listitem>
			<listitem>
				<para>hbase.executorservice.metric.enable</para>
				<para>开启线程池指标上报(设置成true)</para>
			</listitem>
			<listitem>
				<para>hbase.ipc.warn.response.time</para>
				<para>responseTooSlow时间阈值(设置成5000)</para>
			</listitem>
			<listitem>
				<para>hbase.ipc.server.largequery.isolate</para>
				<para>开启大查询隔离(设置成true)</para>
			</listitem>
			<listitem>
				<para>hbase.regionserver.largequery.cacheblock</para>
				<para>大查询开启块缓存(设置成false)</para>
			</listitem>
			<listitem>
				<para>hbase.regionserver.largequery.handler.count</para>
				<para>处理大查询线程数量(设置成10)</para>
			</listitem>
			<listitem>
				<para>hbase.hregion.compacting.memstore.type</para>
				<para>memstore整理策略(设置成BASIC)</para>
			</listitem>
			<listitem>
				<para>hbase.hregion.memstore.chunkpool.indexchunkpercent</para>
				<para>索引chunk池占用百分比(设置成0.1)</para>
			</listitem>
			<listitem>
				<para>hbase.ipc.server.reservoir.minimal.allocating.size</para>
				<para>RPC通信过程中，传递的kv大于该阈值采用池化机制进行管理(设置成1)</para>
			</listitem>
		</itemizedlist>
	</section>
	<section>
		<title>hbase-site.xml(master端)</title>
		<para>TODO</para>
	</section>
	<section>
		<title>hbase-security.xml</title>
		<itemizedlist make='bullet'>
			<listitem>
				<para>hbase.master.keytab.file</para>
				<para>封装principal的keytable路径(设置成/path/to/hbase.keytab)</para>
			</listitem>
			<listitem>
				<para>hbase.master.kerberos.principal</para>
				<para>master的principal(设置成hbase/_HOST@REALM.COM)</para>
			</listitem>
			<listitem>
				<para>hbase.regionserver.keytab.file</para>
				<para>封装principal的keytable路径(设置成/path/to/hbase.keytab)</para>
			</listitem>
			<listitem>
				<para>hbase.regionserver.kerberos.principal</para>
				<para>regionserver的principal(设置成hbase/_HOST@REALM.COM)</para>
			</listitem>
			<listitem>
				<para>hbase.security.authentication</para>
				<para>安全认证类型(设置成kerberos)</para>
			</listitem>
			<listitem>
				<para>hbase.security.authorization</para>
				<para>是否开启安全认证(设置成true)</para>
			</listitem>
			<listitem>
				<para>hbase.rpc.engine</para>
				<para>RPC引擎(设置成org.apache.hadoop.hbase.ipc.SecureRpcEngine)</para>
			</listitem>
			<listitem>
				<para>hbase.security.exec.permission.checks</para>
				<para>开启鉴权(设置成true)</para>
			</listitem>
			<listitem>
				<para>hbase.superuser</para>
				<para>管理员用户(设置成hbase)</para>
			</listitem>
		</itemizedlist>
	</section>
	<section>
		<title>hbase-cluster.xml</title>
		<itemizedlist>
			<listitem>
				<para>hbase.rootdir</para>
				<para>hdfs存储根目录(设置成hdfs://ns/hbase)</para>
			</listitem>
			<listitem>
				<para>hbase.zookeeper.quorum</para>
				<para>ZK地址(设置成hostname,hostname2)</para>
			</listitem>
			<listitem>
				<para>hfile.block.cache.size</para>
				<para>LruBlockCache占用百分比(设置成0.1)</para>
			</listitem>
			<listitem>
				<para>hbase.regionserver.global.memstore.size</para>
				<para>memstore分配空间比(设置成0.35)</para>
			</listitem>
			<listitem>
				<para>hbase.regionserver.offheap.global.memstore.size</para>
				<para>memstore堆外分配空间大小(设置成15360)</para>
			</listitem>
			<listitem>
				<para>hbase.bucketcache.size</para>
				<para>BucketCache大小(设置成2097152)</para>
			</listitem>
			<listitem>
				<para>hbase.bucketcache.ioengine</para>
				<para>BucketCache存储路径(设置成file:/path/to/blockcache/cachedata)</para>
			</listitem>
			<listitem>
				<para>hbase.offpeak.start.hour</para>
				<para>非尖峰时段开始时间点(设置成1)</para>
			</listitem>
			<listitem>
				<para>hbase.offpeak.end.hour</para>
				<para>非尖峰时段结束时间点(设置成4)</para>
			</listitem>
			<listitem>
				<para>hbase.ipc.server.callqueue.read.ratio</para>
				<para>查询handler占比(设置成0.6)</para>
			</listitem>
			<listitem>
				<para>hbase.ipc.server.callqueue.scan.ratio</para>
				<para>查询handler中用于处理Scan的占比(设置成0.1)</para>
			</listitem>
			<listitem>
				<para>hbase.coprocessor.region.classes</para>
				<para>Region级别的协处理器(设置成org.apache.hadoop.hbase.security.access.AccessController, org.apache.hadoop.hbase.security.token.TokenProvider)</para>
			</listitem>
			<listitem>
				<para>hbase.coprocessor.master.classes</para>
				<para>master级别的协处理器(设置成org.apache.hadoop.hbase.security.access.AccessController, org.apache.hadoop.hbase.rsgroup.RSGroupAdminEndpoint)</para>
			</listitem>
			<listitem>
				<para>hbase.coprocessor.regionserver.classes</para>
				<para>RS级别的协处理器(设置成org.apache.hadoop.hbase.security.access.AccessController)</para>
			</listitem>
			<listitem>
				<para>hbase.wal.batch.size</para>
				<para>AsyncWAL模式下每次hflush的buffer大小(设置成1048576)</para>
			</listitem>
			<listitem>
				<para>hbase.regionserver.hlog.sync.timeout</para>
				<para>执行sync(hflush)操作的超时时间(设置成30000)</para>
			</listitem>
			<listitem>
				<para>hbase.wal.storage.policy</para>
				<para>WAL存储策略，结合dngroup特性(设置成HOT/NONE)</para>
			</listitem>
			<listitem>
				<para>hbase.wal.regiongrouping.numgroups</para>
				<para>并发开启的WAL数量(设置成4)</para>
			</listitem>
			<listitem>
				<para>hbase.ipc.server.reservoir.enabled</para>
				<para>RPC通信数据池化管理(设置成true)</para>
			</listitem>
			<listitem>
				<para>hbase.server.allocator.max.buffer.count</para>
				<para>buffer池中的BB数量(设置成29760)</para>
			</listitem>
			<listitem>
				<para>hbase.server.allocator.buffer.size</para>
				<para>buffer池中每个BB的字节大小(设置成66560)</para>
			</listitem>
			<listitem>
				<para>hbase.store.stripe.majorcompaction.enabled</para>
				<para>是否可以针对stripe做大整理(设置成true)</para>
			</listitem>
			<listitem>
				<para>hbase.hfile.prefetch.delay</para>
				<para>多长时间后开始预热HFile的块数据(设置成30000)</para>
			</listitem>
			<listitem>
				<para>hbase.bucketcache.composite.enabled</para>
				<para>是否开启分层BucketCache特性，LruBlockCache替换成BucketCache(设置成true)</para>
			</listitem>
			<listitem>
				<para>hbase.bucketcache.l1.ioengine</para>
				<para>L1Cache的存储引擎(设置成offheap)</para>
			</listitem>
			<listitem>
				<para>hbase.bucketcache.l1.writer.queuelength</para>
				<para>L1Cache每个写队列最大长度(设置成128)</para>
			</listitem>
			<listitem>
				<para>hbase.bucketcache.l1.writer.threads</para>
				<para>L1Cache写线程数量(设置成10)</para>
			</listitem>
			<listitem>
				<para>hbase.bucketcache.l1.size</para>
				<para>L1Cache的大小(设置成40960)</para>
			</listitem>
			<listitem>
				<para>hbase.hstore.checksum.algorithm</para>
				<para>checksum算法(设置成NULL)</para>
			</listitem>
			<listitem>
				<para>hbase.regionserver.checksum.verify</para>
				<para>是否开启checksum校验(设置成false)</para>
			</listitem>
			<listitem>
				<para>hbase.server.parallel.get.enabled</para>
				<para>针对multiget服务端开启多线程查询(设置成true)</para>
			</listitem>
			<listitem>
				<para>hbase.server.multiget.maxrows</para>
				<para>每次multiget最多返回多少行(设置成100)</para>
			</listitem>
			<listitem>
				<para>hbase.server.parallel.get.threads</para>
				<para>处理multiget请求的并发线程数(设置成40)</para>
			</listitem>
			<listitem>
				<para>hbase.rs.prefetch.excludes</para>
				<para>针对哪些操作不开启prefetch(设置成10，二机制对应1010，不包含flush和compaction)</para>
			</listitem>
			<listitem>
				<para>hbase.rs.prefetchcompactedblocksonwrite</para>
				<para>针对整理开启写预热(设置成true)</para>
			</listitem>
			<listitem>
				<para>hbase.hfile.thread.prefetch</para>
				<para>处理HFile预热的线程数(设置成10)</para>
			</listitem>
			<listitem>
				<para>hbase.server.compactchecker.interval.multiplier</para>
				<para>用于控制整理操作的触发周期(设置成60，对应10分钟)</para>
			</listitem>
			<listitem>
				<para>kafka.bootstrap.servers</para>
				<para>kafka的broker地址(设置成hostname:9092,hostname2:9092)</para>
			</listitem>
			<listitem>
				<para>hbase.wal.provider</para>
				<para>wal策略(设置成kafkaclient)</para>
			</listitem>
		</itemizedlist>
	</section>
	<section>
		<title>hbase-cluster.xml(master端)</title>
		<para>TODO</para>
	</section>
	<section>
		<title>hbase-replication.xml</title>
		<itemizedlist>
			<listitem>
				<para>hbase.replication.bulkload.enabled</para>
				<para>是否开启bulkload复制功能(设置成false)</para>
			</listitem>
			<listitem>
				<para>hbase.replication.cluster.id</para>
				<para>原集群唯一标识</para>
			</listitem>
			<listitem>
				<para>hbase.replication.bulkload.copy.maxthreads</para>
				<para>执行HFile拷贝的最大线程数(设置成30)</para>
			</listitem>
			<listitem>
				<para>hbase.replication.bulkload.copy.hfiles.perthread</para>
				<para>每个线程每次最多拷贝的HFile个数(设置成5)</para>
			</listitem>
			<listitem>
				<para>hbase.bulkload.retries.number</para>
				<para>bulkload失败后的重试次数(设置成10)</para>
			</listitem>
			<listitem>
				<para>hbase.bulkload.staging.dir</para>
				<para>目标集群的临时拷贝目录(设置成/tmp/hbase-staging)</para>
			</listitem>
			<listitem>
				<para>hbase.replication.conf.dir</para>
				<para>目标集群的配置信息(设置成/path/to/hbase/conf/replication)</para>
			</listitem>
			<listitem>
				<para>replication.source.shipedits.timeout</para>
				<para>主集群向目标集群同步WALEdit的超时时间(设置成600000)</para>
			</listitem>
			<listitem>
				<para>hbase.replication.bulkload.maxthreads</para>
				<para>开启异步复制功能后，通过该参数配置异步导入线程数(设置成5)</para>
			</listitem>
			<listitem>
				<para>replication.source.nb.capacity</para>
				<para>每次最多同步多少条记录(设置成2500)</para>
			</listitem>
			<listitem>
				<para>replication.source.size.capacity</para>
				<para>每次同步的最大数据量(设置成4194304)</para>
			</listitem>
			<listitem>
				<para>hbase.regionserver.replication.handler.count</para>
				<para>用于处理replication操作的线程数(设置成10)</para>
			</listitem>
			<listitem>
				<para>replication.sink.client.ops.timeout</para>
				<para>ReplicationSink执行batch写操作的超时时间(设置成120000)</para>
			</listitem>
		</itemizedlist>
	</section>
	<section>
		<title>hbase-env.sh</title>
		<programlistingco>
			<programlisting>
export JAVA_HOME=/usr/local/java18
export HADOOP_HOME=/opt/meituan/hadoop
export HBASE_OPTS="-Dio.netty.recycler.maxCapacityPerThread=327680
  -Dio.netty.leakDetection.level=DISABLED
  -Djava.security.auth.login.config=/path/to/zk-jaas.conf
  -Djava.security.krb5.conf=/path/to/krb5.conf"
export HBASE_MANAGES_ZK=false

export HBASE_MASTER_OPTS="$HBASE_MASTER_OPTS -Xmx32g -Xms32g -XX:+UseG1GC
  -XX:+UnlockExperimentalVMOptions -XX:MaxGCPauseMillis=100
  -XX:InitiatingHeapOccupancyPercent=50 -XX:MaxTenuringThreshold=3 -XX:ParallelGCThreads=28
  -XX:G1ConcRefinementThreads=28 -XX:ConcGCThreads=7 -XX:G1ReservePercent=20
  -XX:G1NewSizePercent=10 -XX:G1MaxNewSizePercent=30 -XX:G1MixedGCLiveThresholdPercent=85
  -XX:G1HeapWastePercent=10 -XX:G1MixedGCCountTarget=8 -XX:G1OldCSetRegionThresholdPercent=10
  -XX:+ParallelRefProcEnabled -XX:G1RSetUpdatingPauseTimePercent=10
  -XX:-OmitStackTraceInFastThrow -XX:+PerfDisableSharedMem -XX:-ResizePLAB
  -verbose:gc -Xloggc:/opt/logs/hbase/master.gc.%p.log -XX:+PrintGCDateStamps
  -XX:+PrintGCDetails -Dcom.sun.management.jmxremote.ssl=false
  -Dcom.sun.management.jmxremote.authenticate=false
  -Dcom.sun.management.jmxremote.port=10202 $HBASE_OPTS"

export HBASE_REGIONSERVER_OPTS="$HBASE_REGIONSERVER_OPTS -Xmx32g -Xms32g
  -XX:MaxDirectMemorySize=64g -XX:+UseG1GC -XX:+UnlockExperimentalVMOptions
  -XX:MaxGCPauseMillis=50 -XX:InitiatingHeapOccupancyPercent=70 -XX:MaxTenuringThreshold=3
  -XX:ParallelGCThreads=28 -XX:G1ConcRefinementThreads=28 -XX:ConcGCThreads=7
  -XX:G1ReservePercent=15 -XX:G1NewSizePercent=10 -XX:G1MaxNewSizePercent=30
  -XX:G1MixedGCLiveThresholdPercent=85 -XX:G1HeapWastePercent=10 -XX:G1MixedGCCountTarget=8
  -XX:G1OldCSetRegionThresholdPercent=10 -XX:+ParallelRefProcEnabled
  -XX:G1RSetUpdatingPauseTimePercent=10 -XX:-OmitStackTraceInFastThrow
  -XX:+PerfDisableSharedMem -XX:-ResizePLAB -verbose:gc
  -Xloggc:/opt/logs/hbase/regionserver.gc.%p.log
  -XX:ErrorFile=/opt/logs/hbase/hs_regionserver_err_pid.log -XX:+PrintGCDateStamps
  -XX:+PrintGCDetails -XX:+PrintHeapAtGC -XX:+PrintReferenceGC
  -XX:+PrintTenuringDistribution -XX:+PrintAdaptiveSizePolicy
  -Dcom.sun.management.jmxremote.ssl=false -Dcom.sun.management.jmxremote.authenticate=false
  -Dcom.sun.management.jmxremote.port=10202 $HBASE_OPTS"

export HBASE_LOG_DIR=/opt/logs/hbase
export HBASE_PID_DIR=/opt/logs/hbase/pids
export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$HADOOP_HOME/lib/native/:/usr/local/lib/
export HBASE_LIBRARY_PATH=$HBASE_LIBRARY_PATH:$HBASE_HOME/lib/native:/usr/local/lib/
export HBASE_ROOT_LOGGER=INFO,DRFA
			</programlisting>
		</programlistingco>
	</section>
	<section>
		<title>hadoop-metrics2-hbase.properties</title>
		<programlistingco>
			<programlisting>
hbase.sink.falcon.class=org.apache.hadoop.metrics2.sink.FalconSink
*.period=60
hbase.sink.falcon.url=http://_HOST:1988/v1/push
*.source.filter.class=org.apache.hadoop.metrics2.filter.RegexFilter
*.record.filter.class=${*.source.filter.class}
*.metric.filter.class=${*.source.filter.class}
hbase.*.source.filter.include=
  .*sub=Server|.*sub=IPC|.*sub=IO|.*sub=WAL|.*sub=Replication|.*JvmMetrics|.*sub=Table.*
			</programlisting>
		</programlistingco>
	</section>
	<para>与hdfs相关的常用配置如下：</para>
	<section>
		<title>core-site.xml</title>
		<itemizedlist>
			<listitem>
				<para>fs.trash.interval</para>
				<para>回收站的清理周期(设置成4320)</para>
			</listitem>
			<listitem>
				<para>hadoop.http.staticuser.user</para>
				<para>设置成管理员用户</para>
			</listitem>
			<listitem>
				<para>io.compression.codecs</para>
				<para>支持的压缩算法(设置成org.apache.hadoop.io.compress.DefaultCodec, org.apache.hadoop.io.compress.GzipCodec, org.apache.hadoop.io.compress.BZip2Codec, org.apache.hadoop.io.compress.SnappyCodec, com.hadoop.compression.lzo.LzoCodec, com.hadoop.compression.lzo.LzopCodec)</para>
			</listitem>
			<listitem>
				<para>io.compression.codec.lzo.class</para>
				<para>设置成com.hadoop.compression.lzo.LzoCodec</para>
			</listitem>
			<listitem>
				<para>ipc.client.connect.max.retries</para>
				<para>连接失败最大重试次数(设置成10)</para>
			</listitem>
			<listitem>
				<para>io.serializations</para>
				<para>支持的序列化类(设置成org.apache.hadoop.io.serializer.WritableSerialization, org.apache.hadoop.io.serializer.avro.AvroSpecificSerialization, org.apache.hadoop.io.serializer.avro.AvroReflectSerialization)</para>
			</listitem>
			<listitem>
				<para>net.topology.node.switch.mapping.impl</para>
				<para>机架感知实现类(设置成org.apache.hadoop.net.ScriptBasedMapping)</para>
			</listitem>
			<listitem>
				<para>ipc.server.read.threadpool.size</para>
				<para>reader线程数(设置成10)</para>
			</listitem>
			<listitem>
				<para>hadoop.security.authentication</para>
				<para>安全认证方式(设置成kerberos)</para>
			</listitem>
			<listitem>
				<para>hadoop.security.authorization</para>
				<para>是否启用安全认证(设置成true)</para>
			</listitem>
			<listitem>
				<para>hadoop.security.auth_to_local</para>
				<para>kerberos通过principal解析用户规则(设置成RULE:[1:$1] RULE:[2:$1])</para>
			</listitem>
			<listitem>
				<para>hadoop.http.authentication.type</para>
				<para>设置成kerberos</para>
			</listitem>
			<listitem>
				<para>ha.zookeeper.acl</para>
				<para>ZKFC有关znode的访问权限(设置成sasl:zookeeper:cdwra)</para>
			</listitem>
			<listitem>
				<para>hadoop.proxyuser.hbase.groups</para>
				<para>允许hbase用户代理成哪些用户组(设置成*)</para>
			</listitem>
			<listitem>
				<para>hadoop.proxyuser.hbase.groups</para>
				<para>允许hbase代理用户在哪些机器上访问该服务(设置成*)</para>
			</listitem>
		</itemizedlist>
	</section>
	<section>
		<title>core-cluster.xml</title>
		<itemizedlist>
			<listitem>
				<para>fs.defaultFS</para>
				<para>hdfs根目录空间(设置成hdfs://ns)</para>
			</listitem>
			<listitem>
				<para>ha.zookeeper.quorum</para>
				<para>ZK地址，用于存储ZKFC用到的znode(设置成hostname:2181,hostname2:2181)</para>
			</listitem>
			<listitem>
				<para>net.topology.zone</para>
				<para>集群所在机房(多机房架构用到)</para>
			</listitem>
			<listitem>
				<para>net.topology.impl</para>
				<para>拓扑结构识别类(dngroup特性用到，设置成org.apache.hadoop.net.GroupBasedNetworkTopology)</para>
			</listitem>
			<listitem>
				<para>net.topology.script.file.name</para>
				<para>机架感知脚本(设置成/path/to/group-awareness.sh)</para>
			</listitem>
		</itemizedlist>
	</section>
	<section>
		<title>hdfs-site.xml</title>
		<itemizedlist>
			<listitem>
				<para>dfs.blocksize</para>
				<para>默认块大小(设置成268435456)</para>
			</listitem>
			<listitem>
				<para>dfs.replication</para>
				<para>默认副本数(设置成3)</para>
			</listitem>
			<listitem>
				<para>dfs.hosts.exclude</para>
				<para>用于下线datanode(设置成目标文件路径)</para>
			</listitem>
			<listitem>
				<para>dfs.namenode.handler.count</para>
				<para>NN处理RPC请求的handler数量(设置成200)</para>
			</listitem>
			<listitem>
				<para>dfs.ha.automatic-failover.enabled</para>
				<para>是否启用NN自动热切功能(设置成true)</para>
			</listitem>
			<listitem>
				<para>ha.failover-controller.graceful-fence.rpc-timeout.ms</para>
				<para>设置成80000</para>
			</listitem>
			<listitem>
				<para>ha.failover-controller.graceful-fence.rpc-timeout.ms</para>
				<para>设置成180000</para>
			</listitem>
			<listitem>
				<para>dfs.ha.fencing.methods</para>
				<para>执行fence操作的命令，设置成sshfence(hdfs:22)</para>
			</listitem>
			<listitem>
				<para>dfs.ha.fencing.ssh.private-key-files</para>
				<para>用于连接目标机器的私钥(设置成/home/hdfs/.ssh/id_rsa)</para>
			</listitem>
			<listitem>
				<para>dfs.datanode.fsdataset.volume.choosing.policy</para>
				<para>设置成org.apache.hadoop.hdfs.server.datanode.fsdataset.AvailableSpaceVolumeChoosingPolicy</para>
			</listitem>
			<listitem>
				<para>dfs.datanode.failed.volumes.tolerated</para>
				<para>单DN允许最大坏盘数(设置成3)</para>
			</listitem>
			<listitem>
				<para>dfs.client.read.shortcircuit</para>
				<para>是否启用短路读(设置成true)</para>
			</listitem>
			<listitem>
				<para>dfs.domain.socket.path</para>
				<para>设置成/var/run/hadoop-hdfs/dn._PORT</para>
			</listitem>
			<listitem>
				<para>dfs.namenode.num.extra.edits.retained</para>
				<para>设置成10</para>
			</listitem>
			<listitem>
				<para>dfs.namenode.max.extra.edits.segments.retained</para>
				<para>设置成10</para>
			</listitem>
			<listitem>
				<para>dfs.namenode.acls.enabled</para>
				<para>是否开启ACL过滤(设置成true)</para>
			</listitem>
			<listitem>
				<para>dfs.namenode.datanode.registration.ip-hostname-check</para>
				<para>设置成false</para>
			</listitem>
			<listitem>
				<para>dfs.namenode.checkpoint.period</para>
				<para>StandbyNN触发checkpoint操作的周期(设置成21600)</para>
			</listitem>
			<listitem>
				<para>dfs.namenode.checkpoint.txns</para>
				<para>操作事务数达到该阈值触发checkpoint(设置成50000000)</para>
			</listitem>
			<listitem>
				<para>dfs.datanode.handler.count</para>
				<para>DN处理RPC请求的handler数(设置成100)</para>
			</listitem>
			<listitem>
				<para>dfs.datanode.data.dir.perm</para>
				<para>设置成770</para>
			</listitem>
			<listitem>
				<para>dfs.datanode.max.transfer.threads</para>
				<para>用来做块数据传输的线程数(设置成8192)</para>
			</listitem>
			<listitem>
				<para>dfs.datanode.balance.bandwidthPerSec</para>
				<para>balancer带宽限制(设置成10485760)</para>
			</listitem>
			<listitem>
				<para>dfs.block.local-path-access.user</para>
				<para>设置成hdfs</para>
			</listitem>
			<listitem>
				<para>dfs.image.transfer.timeout</para>
				<para>Standby向Active传输fsimage的超时时间(设置成1800000)</para>
			</listitem>
			<listitem>
				<para>dfs.storage.policy.enabled</para>
				<para>开启分层存储功能(设置成true)</para>
			</listitem>
			<listitem>
				<para>ignore.secure.ports.for.testing</para>
				<para>设置成true</para>
			</listitem>
			<listitem>
				<para>dfs.datanode.du.reserved</para>
				<para>每块磁盘预留空间(设置成10737418240)</para>
			</listitem>
			<listitem>
				<para>dfs.namenode.fs-limits.max-directory-items</para>
				<para>hdfs单个目录可以存放的文件数量(设置成6000000)</para>
			</listitem>
			<listitem>
				<para>dfs.client.socket-timeout</para>
				<para>客户端读写块超时时间(设置成120000)</para>
			</listitem>
		</itemizedlist>
	</section>
	<section>
		<title>hdfs-cluster.xml</title>
		<itemizedlist>
			<listitem>
				<para>dfs.nameservices</para>
				<para>hdfs命名空间</para>
			</listitem>
			<listitem>
				<para>dfs.ha.namenodes.&lt;ns></para>
				<para>目标命名空间下面包含哪些nn(设置成nn1,nn2)</para>
			</listitem>
			<listitem>
				<para>dfs.namenode.rpc-address.&lt;ns>.nn1</para>
				<para>nn1的rpcServer地址(设置成hostname-nn1:8020)</para>
			</listitem>
			<listitem>
				<para>dfs.namenode.rpc-address.&lt;ns>.nn2</para>
				<para>nn2的rpcServer地址(设置成hostname-nn2:8020)</para>
			</listitem>
			<listitem>
				<para>dfs.namenode.http-address.&lt;ns>.nn1</para>
				<para>nn1的httpServer地址(设置成hostname-nn1:50070)</para>
			</listitem>
			<listitem>
				<para>dfs.namenode.http-address.&lt;ns>.nn2</para>
				<para>nn2的httpServer地址(设置成hostname-nn2:50070)</para>
			</listitem>
			<listitem>
				<para>dfs.namenode.shared.edits.dir</para>
				<para>设置成qjournal://jn1:8485;jn2:8485;jn3:8485/&lt;ns></para>
			</listitem>
			<listitem>
				<para>dfs.client.failover.proxy.provider.&lt;ns></para>
				<para>设置成org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider</para>
			</listitem>
			<listitem>
				<para>dfs.datanode.data.dir</para>
				<para>[DISK]/data1/hadoop/dfs/data,[SSD]/data2/hadoop/dfs/data</para>
			</listitem>
			<listitem>
				<para>dfs.namenode.name.dir</para>
				<para>namenode保存fsimage和editlog的地址(设置成file:///opt/hadoop/dfs/name)</para>
			</listitem>
			<listitem>
				<para>dfs.journalnode.edits.dir</para>
				<para>JN数据的目录地址(设置成/opt/hadoop/dfs/journal/)</para>
			</listitem>
			<listitem>
				<para>dfs.block.replicator.classname</para>
				<para>副本分发策略(设置成org.apache.hadoop.hdfs.server.blockmanagement.GroupBasedPlacementPolicy)</para>
			</listitem>
			<listitem>
				<para>dfs.http.acl.white.list</para>
				<para>允许哪些机器动态修改LOG级别(设置成hostname1,hostname2)</para>
			</listitem>
			<listitem>
				<para>dfs.client.socketcache.capacity</para>
				<para>设置成64</para>
			</listitem>
			<listitem>
				<para>dfs.client.read.shortcircuit.streams.cache.size</para>
				<para>设置成512</para>
			</listitem>
			<listitem>
				<para>dfs.namenode.replication.considerLoad</para>
				<para>执行DN选块过程中是考虑每台DN的负载情况(设置成false，dngroup场景不考虑)</para>
			</listitem>
			<listitem>
				<para>dfs.checksum.type</para>
				<para>checksum算法(设置成NULL，不启用)</para>
			</listitem>
			<listitem>
				<para>dfs.client.write.byte-array-manager.enabled</para>
				<para>针对DFSPacket写入开启池化管理(设置成true)</para>
			</listitem>
			<listitem>
				<para>dfs.client.write.byte-array-manager.count-threshold</para>
				<para>The count threshold for each array length so that a manager is created only after the allocation count exceeds the threshold(设置成128)</para>
			</listitem>
			<listitem>
				<para>dfs.client.write.byte-array-manager.count-limit</para>
				<para>The maximum number of arrays allowed for each array length(设置成2048)</para>
			</listitem>
			<listitem>
				<para>dfs.client.write.byte-array-manager.count-reset-time-period-ms</para>
				<para>he time period in milliseconds that the allocation count for each array length is reset to zero if there is no increment(设置成10000)</para>
			</listitem>
		</itemizedlist>
	</section>
	<section>
		<title>hdfs-security.xml</title>
		<itemizedlist>
			<listitem>
				<para>dfs.block.access.token.enable</para>
				<para>设置成true</para>
			</listitem>
			<listitem>
				<para>dfs.permissions.superusergroup</para>
				<para>管理员账号(设置成hdfs)</para>
			</listitem>
			<listitem>
				<para>dfs.namenode.kerberos.principal</para>
				<para>设置成hdfs/_HOST@REALM.COM</para>
			</listitem>
			<listitem>
				<para>dfs.namenode.keytab.file</para>
				<para>设置成path/to/hdfs.keytab</para>
			</listitem>
			<listitem>
				<para>dfs.datanode.kerberos.principal</para>
				<para>设置成hdfs/_HOST@SANKUAI.COM</para>
			</listitem>
			<listitem>
				<para>dfs.datanode.keytab.file</para>
				<para>设置成path/to/hdfs.keytab</para>
			</listitem>
			<listitem>
				<para>dfs.web.authentication.kerberos.principal</para>
				<para>设置成HTTP/_HOST@REALM.COM</para>
			</listitem>
			<listitem>
				<para>dfs.web.authentication.kerberos.keytab</para>
				<para>设置成path/to/hdfs.keytab</para>
			</listitem>
			<listitem>
				<para>dfs.journalnode.keytab.file</para>
				<para>设置成path/to/hdfs.keytab</para>
			</listitem>
			<listitem>
				<para>dfs.journalnode.kerberos.principal</para>
				<para>设置成hdfs/_HOST@REALM.COM</para>
			</listitem>
			<listitem>
				<para>dfs.journalnode.kerberos.internal.spnego.principal</para>
				<para>设置成HTTP/_HOST@REALM.COM</para>
			</listitem>
			<listitem>
				<para>dfs.cluster.administrators</para>
				<para>设置成hdfs</para>
			</listitem>
			<listitem>
				<para>dfs.namenode.kerberos.internal.spnego.principal</para>
				<para>设置成HTTP/_HOST@REALM.COM</para>
			</listitem>
		</itemizedlist>
	</section>
	<section>
		<title>hadoop-env.sh</title>
		<programlistingco>
			<programlisting>
export JAVA_HOME=/usr/local/java18
export JSVC_HOME=/opt/meituan/hadoop/libexec/commons-daemon/unix
export HADOOP_CONF_DIR=${HADOOP_CONF_DIR:-"/etc/hadoop"}

for f in $HADOOP_HOME/contrib/capacity-scheduler/*.jar; do
  if [ "$HADOOP_CLASSPATH" ]; then
    export HADOOP_CLASSPATH=$HADOOP_CLASSPATH:$f
  else
    export HADOOP_CLASSPATH=$f
  fi
done

export HADOOP_OPTS="$HADOOP_OPTS -Djava.net.preferIPv4Stack=true
  -Djava.security.krb5.conf=/path/to/krb5.conf"
export HADOOP_JMX_BASE="-Dcom.sun.management.jmxremote.ssl=false
  -Dcom.sun.management.jmxremote.authenticate=false"

export HADOOP_NAMENODE_OPTS="-Xmx60g -Xms60g -Xmn10g -XX:PermSize=1g
  -XX:MaxPermSize=1g -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=75
  -XX:+UseCMSCompactAtFullCollection -XX:+CMSParallelRemarkEnabled
  -XX:CMSFullGCsBeforeCompaction=2 -XX:+CMSScavengeBeforeRemark
  -XX:+CMSClassUnloadingEnabled -verbose:gc
  -Xloggc:/opt/logs/hadoop/hdfs/namenode-gc-%p.out -XX:+PrintGCDateStamps
  -XX:+PrintGCDetails -XX:+HeapDumpOnOutOfMemoryError -Dhadoop.root.logger=INFO,DRFA
  -Dhadoop.security.logger=${HADOOP_SECURITY_LOGGER:-INFO,RFAS}
  -Dhdfs.audit.logger=${HDFS_AUDIT_LOGGER:-INFO,NullAppender}
  -Dcom.sun.management.jmxremote.ssl=false -Dcom.sun.management.jmxremote.authenticate=false
  -Dcom.sun.management.jmxremote.port=10001 -Djava.net.preferIPv4Stack=true
  -Djava.security.krb5.conf=/path/to/krb5.conf"

export HADOOP_DATANODE_OPTS="-Xmx3g -Xms3g -XX:+UseG1GC -XX:+UnlockExperimentalVMOptions
  -XX:MaxGCPauseMillis=50 -XX:InitiatingHeapOccupancyPercent=65 -XX:MaxTenuringThreshold=3
  -XX:ParallelGCThreads=24 -XX:G1ConcRefinementThreads=24 -XX:ConcGCThreads=6
  -XX:G1ReservePercent=10 -XX:G1NewSizePercent=10 -XX:G1MaxNewSizePercent=15
  -XX:G1MixedGCLiveThresholdPercent=85 -XX:G1HeapWastePercent=10 -XX:G1MixedGCCountTarget=8
  -XX:G1OldCSetRegionThresholdPercent=10 -XX:+ParallelRefProcEnabled
  -XX:G1RSetUpdatingPauseTimePercent=10 -XX:-OmitStackTraceInFastThrow
  -XX:+PerfDisableSharedMem -XX:-ResizePLAB
  -verbose:gc -Xloggc:/opt/logs/hadoop/hdfs/datanode-gc-%p.out
  -XX:+PrintGCDateStamps -XX:+PrintGCDetails -Dhadoop.root.logger=INFO,DRFA
  -Dhadoop.security.logger=ERROR,RFAS
  -Dcom.sun.management.jmxremote.ssl=false -Dcom.sun.management.jmxremote.authenticate=false
  -Dcom.sun.management.jmxremote.port=10002 -Djava.net.preferIPv4Stack=true
  -Djava.security.krb5.conf=/path/to/krb5.conf"

export HADOOP_ZKFC_OPTS="-Xms2g -Xmx2g
  -Djava.security.auth.login.config=/path/to/zk-jaas.conf $HADOOP_OPTS"
export HADOOP_SECONDARYNAMENODE_OPTS="
  -Dhadoop.security.logger=${HADOOP_SECURITY_LOGGER:-INFO,RFAS}
  -Dhdfs.audit.logger=${HDFS_AUDIT_LOGGER:-INFO,NullAppender} $HADOOP_SECONDARYNAMENODE_OPTS"
export HADOOP_NFS3_OPTS="$HADOOP_NFS3_OPTS"
export HADOOP_PORTMAP_OPTS="-Xmx512m $HADOOP_PORTMAP_OPTS"
export HADOOP_CLIENT_OPTS="-Xmx512m $HADOOP_CLIENT_OPTS"
export HADOOP_SECURE_DN_USER=hdfs
export HADOOP_LOG_DIR=/opt/logs/hadoop/${HADOOP_SECURE_DN_USER}
export HADOOP_SECURE_DN_LOG_DIR=${HADOOP_LOG_DIR}/${HADOOP_HDFS_USER}
export HADOOP_PID_DIR=${HADOOP_LOG_DIR}/pids
export HADOOP_SECURE_DN_PID_DIR=${HADOOP_PID_DIR}
export HADOOP_IDENT_STRING=$USER
			</programlisting>
		</programlistingco>
	</section>
	<section>
		<title>zk-jaas.conf</title>
		<programlisting>
			<programlistingco>
Client {
    com.sun.security.auth.module.Krb5LoginModule required
    useKeyTab=true
    keyTab="/path/to/zookeeper.keytab"
    storeKey=true
    useTicketCache=false
    principal="zookeeper/hostname@REALM.COM";
};
			</programlistingco>
		</programlisting>
	</section>
</section>