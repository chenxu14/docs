<?xml version="1.0" encoding="UTF-8"?>
<section version="5.0" xmlns="http://docbook.org/ns/docbook"
	xmlns:xlink="http://www.w3.org/1999/xlink"
	xmlns:xi="http://www.w3.org/2001/XInclude"
	xmlns:ns5="http://www.w3.org/2000/svg"
	xmlns:ns4="http://www.w3.org/1998/Math/MathML"
	xmlns:ns3="http://www.w3.org/1999/xhtml"
	xmlns:db="http://docbook.org/ns/docbook">
	<title>WAL解耦到KAFKA</title>
	<para>HBase原生的WAL实现主要是基于HDFS来进行存储的，写入操作由RegionServer端触发，大致的写入流程如图所示：</para>
	<mediaobject>
		<imageobject>
			<imagedata contentdepth="100%" width="90%" fileref="../media/hbase/wal_hdfs.png"></imagedata>
		</imageobject>
	</mediaobject>
	<para>可以看到WAL的保存操作是在RS端进行的，当收到客户端的put请求之后，首先将数据内容写入WAL，以便机器宕机时做相应的容灾恢复处理，然后在将数据保存到memstore，当memstore达到一定阈值之后，持久化到HFile中进行存储。</para>
	<para>基于该方式做数据写入目前主要存在以下弊端：当RS进程所在机器出现宕机时，需要经历很长时间的MTTR过程(大致分钟级)，在此期间相关Region的访问是被拒绝的。虽然HBase后续引入了Replica特性来为每个Region声明额外的副本，但是副本Region只能接管读操作，无法对写操作进行接管，造成客户端写操作出现分钟级的响应延迟。</para>
	<para>为此我们可以考虑将WAL的写操作从HBase集群中解耦出去，与memstore的写操作异步进行，这样即使RS出现宕机也不会影响客户端的写入流程，重构后的写链路如图所示：</para>
	<mediaobject>
		<imageobject>
			<imagedata contentdepth="100%" width="90%" fileref="../media/hbase/wal_kafka.png"></imagedata>
		</imageobject>
	</mediaobject>
	<para>WAL写操作不在由RS端触发，而是交由客户端负责去写；数据内容也不在保存到HDFS，而是基于kafka来做数据存储。kafka针对单个broker的宕机是有相应的容灾恢复能力的，原broker宕机后，处在ISR列表中的其他broker可迅速接管之前的写操作，这样便有效缓解了客户端写操作因MTTR时间过长而导致的响应延迟变慢的问题。</para>
    <para>另外解耦之后还能带来如下额外的好处：</para>
    <orderedlist>
		<listitem>
			<para>kafka的写入效率是要优于HDFS的</para>
			<para>HDFS主要是基于pipeline的方式来进行串行的数据写入，而kafka是所有follower并发向leader做数据拉取，并可设置相应的ack值来降低慢节点对写链路的影响。</para>
		</listitem>
		<listitem>
			<para>为RS进程节省出更多的IO资源</para>
			<para>包括WAL的写IO以及Replication的读IO。</para>
		</listitem>
		<listitem>
			<para>WAL持久化到kafka之后更有利于衍生出其他应用</para>
			<para>比如提供日志审计功能，将数据同步ES/SOLR实现全文检索、二级索引应用，或者将数据同步到DeltaLake实现实时数仓应用。</para>
		</listitem>
	</orderedlist>
    <para>当然解耦之后也会带来相应的弊端，由于写WAL和写memstore异步进行，而kafka向HBase同步数据会有一定的延迟，导致数据访问只能做到最终一致性，对于写入即可见的强一致性需求则无法满足。</para>
    <section>
    	<title>基于kafka的WALProvider实现</title>
    	<para>HBase原生对外提供了2种类型的WALProvider，都是基于HDFS来做数据存储的，一种是基于pipeline的写入方式(FSHLogProvider)，另一种是基于扇出的写入方式(AsyncFSWALProvider)。在将WAL转存到kafka之后，我们需要引入与之相应的WALProvider来满足以下功能需求。</para>
    	<orderedlist>
			<listitem>
				<para>将WAL的sequenceId和MVCC的事务ID进行解耦</para>
				<para>原生实现里，WAL的sequenceId和MVCC的事务ID是采用同一套ID生成器的，由于WAL的写入和memstore的写入处在同一个事务里，采用相同的计数ID可以让应用变得更加简洁。</para>
				<para>然而在WAL解耦到kafka之后，写WAL和写memstore变成了一个异步的过程，并且分散在不同的进程中来完成，在想使用同一套ID生成器变得不太可能，因此我们需要将WAL的sequenceId从原生的计数逻辑中解耦出来，不在与MVCC公用同一套ID生成器，而是将其与kafka的分区偏移量进行绑定，以便在执行LogSplit操作时通过指定偏移量来对消息进行回溯处理。</para>
				<para>WAL的sequenceId主要用于服务LogSplit操作，RS端是通过SequenceIdAccounting来跟踪每个Region最后一次flush时的sequenceId的，并且以心跳通信的方式将其汇报到HMaster端进行保存，以便HMaster在处理LogSplit操作时能够过滤掉已被持久化的无用记录。为此我们可以考虑将kafka的消费偏移量从Consumer端传递到RS端，使其能够汇总到SequenceIdAccounting中进行保存，这样便可以复用已有的心跳汇报流程，整个OFFSET的传递过程如图所示：</para>
                <mediaobject>
					<imageobject>
						<imagedata contentdepth="100%" width="90%" fileref="../media/hbase/offset_pass.png"></imagedata>
					</imageobject>
				</mediaobject>
				<para>Consumer在执行kafka消费过程中，可以拿到每条WAL日志记录对应的offset，然后通过执行Mutation#setAttribute方法将其序列化到Mutation对象实例中进行保存，这样HBaseClient在执行RPC远程调用时便可将其发送到RegionServer端进行处理。</para>
				<para>RegionServer端收到请求之后，可通过Mutation#getAttribute方法将offset信息反序列化出来，在通过自定义的KafkaClientWAL将其传入SequenceIdAccounting中进行统计。KafkaClientWAL与原生WAL最大的区别便是这里只需记录每条日志的offset而无需对日志内容进行保存，因为保存操作已经在客户端完成。而我们基于kafka所实现的WALProvider主要目的便是用来创建出KafkaClientWAL对象实例。</para>
			</listitem>
			<listitem>
				<para>针对Marker记录需要保留原生的WAL写入方式</para>
				<para>WAL除了保存真正的KV数据之外，还会记录一些与操作有关的Marker，这些Marker主要起到以下作用：</para>
				<itemizedlist make='bullet'>
					<listitem>
						<para>用于Replica特性支持</para>
						<para>比如在执行flush操作时会写入FlushMarker，以便副本Region对新生成的HFile进行加载。</para>
					</listitem>
					<listitem><para>用于BulkloadReplication特性支持，详细参考HBASE-13153</para></listitem>
					<listitem>
						<para>修复类似于HBASE-2231的相关问题，问题场景描述如下：</para>
						<para>RS在执行整理操作过程中(生成新的HFile，删除老HFIle之前)，如果其与当前集群发生了失联，则HMaster会将其上部署的Region进行重新的分配处理(假设将其分配到了RS-new)，此时新分配的Region将会同时包含新老两份文件。</para>
						<para>虽然RS与集群发生了失联，但其内部进程依然在工作，如果此时继续之前的整理操作(删除老HFile)会导致Region在RS-new上出现FNFE异常。对此HBASE-2231提供了如下解决方案：在删除老HFile之前先写Marker，如果RS失联，其对应的WAL租约会被HMaster抢占(执行rename操作)，这样RS在写marker时将会失败，从而退出进程。</para>
					</listitem>
				</itemizedlist>
				<para>由于类似HBASE-2231的问题导致Marker写原生WAL的逻辑不能被忽略，因此我们新引入的KafkaClientWAL需要兼容这部分功能：如果是Marker记录将其保存到FSHLog，否则只对offset信息进行记录。</para>
			</listitem>
		</orderedlist>
    </section>
    <section>
    	<title>LogSplit改造</title>
    </section>
    <section>
    	<title>其他变动调整</title>
    	<orderedlist>
    		<listitem>
    			<para>心跳汇报逻辑</para>
    			<para>每个Region最后flush时的sequenceId是通过RS与HMaster的不断心跳来维持上报的，HMaster收到信息以后会对其进行持久化存储(HBASE-20727)，以便在执行LogSplit操作时确定数据回放的起始区间。</para>
    			<para>在原生的汇报逻辑里面，MVCC的事务ID与WAL的sequenceId还尚未解耦，因此每次心跳汇报其实是每个Region最后flush时的MVCC信息(代码逻辑可参考HRegion的setCompleteSequenceId方法)。WAL解耦之后需要对汇报逻辑进行相应调整，如果某个Store从加载至今还没有触发过flush操作，将其对应的sequenceId设置成0进行汇报；否则通过KafkaClientWAL#getEarliestMemstoreSeqNum方法获取其最后flush时的sequenceId进行汇报。另外在Region的所有Store中取sequenceId最大的值来作为当前Region的sequenceId进行汇报。</para>
    		</listitem>
    		<listitem>
    			<para>Region加载逻辑</para>
    		</listitem>
    	</orderedlist>
    </section>
</section>
