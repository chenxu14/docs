<?xml version="1.0" encoding="UTF-8"?>
<section version="5.0" xmlns="http://docbook.org/ns/docbook"
	xmlns:xlink="http://www.w3.org/1999/xlink"
	xmlns:xi="http://www.w3.org/2001/XInclude"
	xmlns:ns5="http://www.w3.org/2000/svg"
	xmlns:ns4="http://www.w3.org/1998/Math/MathML"
	xmlns:ns3="http://www.w3.org/1999/xhtml"
	xmlns:db="http://docbook.org/ns/docbook">
	<title>启用滚动升级</title>
	<orderedlist>
		<listitem>
			<para>NodeManager端添加如下配置项</para>
			<itemizedlist make='bullet'>
				<listitem>
					<para>yarn.nodemanager.recovery.enabled</para>
					<para>将参数值设置成true，默认为false，表示开启NM端的work_preserving功能。</para>
				</listitem>
				<listitem>
					<para>yarn.nodemanager.recovery.dir</para>
					<para>NM用来保存App状态的本地存储路径(基于LevelDB进行存储)，默认值为$hadoop.tmp.dir/yarn-nm-recovery。</para>
				</listitem>
				<listitem>
					<para>yarn.nodemanager.address</para>
					<para>为NM指定固定的端口号，以确保其重启之后已有客户端的通信不受影响，比如可将参数值设置为0.0.0.0:45454。</para>
				</listitem>
				<listitem>
					<para>mapreduce.shuffle.port</para>
					<para>通过该参数为MapReduce的shuffle辅助服务指定固定的端口号，以此来确保服务重新启动之后已有通信不受影响。</para>
				</listitem>
			</itemizedlist>
		</listitem>
		<listitem>
			<para>ResourceManager端添加如下配置项</para>
			<itemizedlist make='bullet'>
				<listitem>
					<para>yarn.resourcemanager.recovery.enabled</para>
					<para>将参数值设置成true，默认为false，表示开启RM端的work_preserving功能。</para>
				</listitem>
				<listitem>
					<para>yarn.resourcemanager.work-preserving-recovery.scheduling-wait-ms</para>
					<para>recover操作的等待时间，RM在这段时间内不会分配新的Container，而是等待已有作业的Container能够恢复，默认值为10000。</para>
				</listitem>
				<listitem>
					<para>yarn.resourcemanager.store.class</para>
					<para>RM端每个App状态的存储媒介，这里基于ZK进行存储，将参数值设置为org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore。</para>
				</listitem>
				<listitem>
					<para>yarn.resourcemanager.zk-address</para>
					<para>Zookeeper集群的通信地址，比如：127.0.0.1:2181,127.0.0.1:2182</para>
				</listitem>
				<listitem>
					<para>yarn.resourcemanager.zk-state-store.parent-path</para>
					<para>将状态数据保存到哪个ZK节点下面，默认值为/rmstore。</para>
				</listitem>
				<listitem>
					<para>yarn.resourcemanager.zk-timeout-ms</para>
					<para>Zookeeper的session超时时间，默认值为10000(10秒).</para>
				</listitem>
				<listitem>
					<para>yarn.resourcemanager.zk-acl</para>
					<para>ZK节点的acl信息，默认值为world:anyone:rwcda，表示任何用户具备所有权限。</para>
				</listitem>
				<listitem>
					<para>yarn.resourcemanager.zk-auth</para>
					<para>ZK启用acl之后，通过该参数来为ZK客户端进行授权。</para>
				</listitem>
			</itemizedlist>
		</listitem>
		<listitem>
			<para>客户端添加如下配置</para>
			<para>默认情况下MR作业所依赖的jar资源都是在NM本地进行部署的(部署在其$HADOOP_HOME/share/hadoop目录下)，这样当NM升级以后，已运行的Container进程会有依赖jar版本前后不一致的问题。为此需要将MR作业所依赖的jar资源单独进行部署，这可通过DistributeCache来实现，具体步骤如下：</para>
			<itemizedlist>
				<listitem>
					<para>首先通过如下命令将MR作业所依赖的jar资源打包成tar文件。</para>
					<para>mvn package -Pdist -DskipTests -Dtar</para>
				</listitem>
				<listitem>
					<para>然后将目标tar文件上传到HDFS，并通过如下参数进行引用。</para>
					<para>mapreduce.application.framework.path参数值设置成hdfs://path/to/hadoop-{version}.tar.gz#mr-framework。</para>
				</listitem>
				<listitem>
					<para>最后将mapreduce.application.classpath参数设置成如下属性值。</para>
					<para>$PWD/mr-framework/hadoop/share/hadoop/mapreduce/*:$PWD/mr-framework/hadoop/share/hadoop/mapreduce/lib/*:$PWD/mr-framework/hadoop/share/hadoop/common/*:$PWD/mr-framework/hadoop/share/hadoop/common/lib/*:$PWD/mr-framework/hadoop/share/hadoop/yarn/*:$PWD/mr-framework/hadoop/share/hadoop/yarn/lib/*:$PWD/mr-framework/hadoop/share/hadoop/hdfs/*:$PWD/mr-framework/hadoop/share/hadoop/hdfs/lib/*:$PWD/mr-framework/hadoop/etc/hadoop</para>
				</listitem>
			</itemizedlist>
			<para>通过以上处理后，MR作业所依赖的jar资源不再使用NM本地的部署，而是从HDFS上进行下载，并将其作为public资源与其他MR作业进行共享。</para>
		</listitem>
	</orderedlist>
</section>