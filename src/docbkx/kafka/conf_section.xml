<?xml version="1.0" encoding="UTF-8"?>
<section version="5.0" xmlns="http://docbook.org/ns/docbook"
	xmlns:xlink="http://www.w3.org/1999/xlink"
	xmlns:xi="http://www.w3.org/2001/XInclude"
	xmlns:ns5="http://www.w3.org/2000/svg"
	xmlns:ns4="http://www.w3.org/1998/Math/MathML"
	xmlns:ns3="http://www.w3.org/1999/xhtml"
	xmlns:db="http://docbook.org/ns/docbook">
	<title>常用配置</title>
	<section>
		<title>Broker端</title>
		<orderedlist>
			<listitem>
				<para>server.properties</para>
				<programlistingco>
					<programlisting>
# The number of threads that the server uses for receiving requests from the network and
# sending responses to the network, The default value for this is 3
num.network.threads=10
# The number of threads that the server uses for processing requests, which may include
# disk I/O, The default value for this is 8.
num.io.threads=10

# The send buffer (SO_SNDBUF) used by the socket server
socket.send.buffer.bytes=102400
# The receive buffer (SO_RCVBUF) used by the socket server
socket.receive.buffer.bytes=102400
# The maximum size of a request that the socket server will accept(protection against OOM)
socket.request.max.bytes=104857600
# A comma separated list of directories under which to store log files
log.dirs=/data10/hadoop/dfs/kafka,/data11/hadoop/dfs/kafka,/data12/hadoop/dfs/kafka
# The number of threads per data directory to be used for log recovery at startup and
# flushing at shutdown. This value is recommended to be increased for installations with
# data dirs located in RAID array.
num.recovery.threads.per.data.dir=1

# default replication factors for automatically created topics
default.replication.factor=3
# Enable auto creation of topic on the server
auto.create.topics.enable=true
# Enables delete topic.
delete.topic.enable＝true
# The default number of log partitions per topic.
num.partitions=8

# The replication factor for topics "__consumer_offsets" and "__transaction_state"
offsets.topic.replication.factor=3
transaction.state.log.replication.factor=3
transaction.state.log.min.isr=2

# The minimum age of a log file to be eligible for deletion due to age
log.retention.hours=168
# When this size is reached a new log segment will be created.
log.segment.bytes=1073741824

# The interval at which log segments are checked to see if they can be deleted according
# to the retention policies
log.retention.check.interval.ms=300000

# ZK related
zookeeper.connect=host1:prot1,host2:prot2
zookeeper.session.timeout.ms=60000

# The following configuration specifies the time, in milliseconds, that the GroupCoordinator
# will delay the initial consumer rebalance. The default value for this is 3 seconds.
group.initial.rebalance.delay.ms=10000

# A typical scenario would be to create a topic with a replication factor of 3, set
# min.insync.replicas to 2, and produce with acks of "all". This will ensure that the
producer raises an exception if a majority of replicas do not receive a write.
min.insync.replicas=2

# Number of fetcher threads used to replicate messages from a source broker. Increasing
# this value can increase the degree of I/O parallelism in the follower broker
num.replica.fetchers=4

# The number of queued requests allowed for data-plane, before blocking the network threads
queued.max.requests=5000

# max wait time for each fetcher request issued by follower replicas.
# This value should always be less than the replica.lag.time.max.ms at all times to prevent
# frequent shrinking of ISR for low throughput topics, The default value for this is 500
replica.fetch.wait.max.ms=100
# If a follower hasn't sent any fetch requests or hasn't consumed up to the leaders log end
# offset for at least this time, the leader will remove the follower from isr
replica.lag.time.max.ms=10000

# The configuration controls the maximum amount of time the client will wait for the
# response of a request. If the response is not received before the timeout elapses
# the client will resend the request or fail the request if retries are exhausted.
request.timeout.ms=30000

# If all replicas are down for a partition, Kafka, by default, chooses first replica that
# comes alive as the leader
unclean.leader.election.enable=false
metric.reporters=com.linkedin.kafka.cruisecontrol.metricsreporter.CruiseControlMetricsReporter
					</programlisting>
				</programlistingco>
			</listitem>
			<listitem>
				<para>kafka-run-class.sh</para>
				<programlistingco>
					<programlisting>
export JAVA_HOME=/usr/local/java18
...
# Generic jvm settings you want to add
if [ -z "$KAFKA_OPTS" ]; then
  KAFKA_OPTS="-Djava.security.krb5.conf=path/to/krb5.conf
      -Djava.security.auth.login.config=path/to/zk-jaas.conf"
fi
...
# JVM performance options
if [ -z "$KAFKA_JVM_PERFORMANCE_OPTS" ]; then
  KAFKA_JVM_PERFORMANCE_OPTS="-server -XX:+UseG1GC -XX:+UnlockExperimentalVMOptions
    -XX:MaxGCPauseMillis=50 -XX:InitiatingHeapOccupancyPercent=70 -XX:MaxTenuringThreshold=3
    -XX:ParallelGCThreads=32 -XX:G1ConcRefinementThreads=32 -XX:ConcGCThreads=8
    -XX:G1ReservePercent=15 -XX:G1NewSizePercent=10 -XX:G1MaxNewSizePercent=30
    -XX:G1MixedGCLiveThresholdPercent=85 -XX:G1HeapWastePercent=10
    -XX:G1MixedGCCountTarget=8 -XX:G1OldCSetRegionThresholdPercent=10
    -XX:+ParallelRefProcEnabled -XX:G1RSetUpdatingPauseTimePercent=10
    -XX:-OmitStackTraceInFastThrow -XX:+PerfDisableSharedMem -XX:-ResizePLAB
    -verbose:gc -Xloggc:/path/to/gc.%p.log -XX:+PrintGCDateStamps -XX:+PrintGCDetails
    -XX:+PrintHeapAtGC -XX:+PrintReferenceGC -XX:+PrintTenuringDistribution
    -XX:+PrintAdaptiveSizePolicy -XX:+ExplicitGCInvokesConcurrent -Djava.awt.headless=true"
fi
					</programlisting>
				</programlistingco>
			</listitem>
			<listitem>
				<para>kafka-server-start.sh</para>
				<programlistingco>
					<programlisting>
...
if [ "x$KAFKA_HEAP_OPTS" = "x" ]; then
    export KAFKA_HEAP_OPTS="-Xmx10G -Xms10G"
fi
...
					</programlisting>
				</programlistingco>
			</listitem>
			<listitem>
				<para>zk-jaas.conf</para>
				<programlistingco>
					<programlisting>
Client {
    com.sun.security.auth.module.Krb5LoginModule required
    useKeyTab=true
    useTicketCache=false
    keyTab="/etc/hadoop/keytabs/hdfs.keytab"
    principal="hdfs/_HOST@REALM.COM";
};
					</programlisting>
				</programlistingco>
			</listitem>
		</orderedlist>
	</section>
	<section>
		<title>Producer端</title>
		<programlistingco>
			<programlisting>
bootstrap.servers=host1:port1,host2:port2
# The number of acknowledgments the producer requires the leader to have received
# before considering a request complete, value can be [all, 0, 1]
acks=all
# 通过该参数决定最终的超时时间, 默认120秒
delivery.timeout.ms=35000

# 单次RPC请求的超时时间，超过该时间请求将重试，直至总时间达到delivery.timeout.ms
# This should be larger than replica.lag.time.max.ms
request.timeout.ms=15000

# Serializer class that implements the org.apache.kafka.common.serialization.Serializer
# 使用ByteBufferSerializer可考虑引入KAFKA-9149补丁，可提升Producer端的GC效率
key.serializer=org.apache.kafka.common.serialization.ByteBufferSerializer
value.serializer=org.apache.kafka.common.serialization.ByteBufferSerializer

# The maximum size of a request in bytes.
max.request.size=1048576

# A list of classes to use as metrics reporters, implementing the MetricsReporter
metric.reporters=org.apache.kafka.common.metrics.MetricsReporter
			</programlisting>
		</programlistingco>
	</section>
	<section>
		<title>Connector相关</title>
		<orderedlist>
			<listitem>
				<para>connect-distributed.properties</para>
				<programlistingco>
					<programlisting>
bootstrap.servers=host1:port1,host2:port2
# unique name for the cluster, used in forming the Connect cluster group. Note that this
# must not conflict with consumer group IDs
group.id=hbase-sink-connect

# specify the format of data in Kafka and how to translate it into Connect data
key.converter=org.apache.kafka.connect.converters.ByteArrayConverter
value.converter=org.apache.kafka.connect.converters.ByteArrayConverter
key.converter.schemas.enable=false
value.converter.schemas.enable=false

# Topic to use for storing offsets, should be replicated and compacted.
offset.storage.topic=connect-offsets
offset.storage.replication.factor=3
offset.storage.partitions=10

# Topic to use for storing connector and task configurations
# should be a single partition, highly replicated and compacted topic
config.storage.topic=connect-configs
config.storage.replication.factor=3

# Topic to use for storing statuses, should be replicated and compacted.
status.storage.topic=connect-status
status.storage.replication.factor=3
status.storage.partitions=5

# 每隔多久提交一次offset
offset.flush.interval.ms=20000

# Set to a list of filesystem paths separated by commas
# to enable class loading isolation for plugins
plugin.path=/path/to/hbase-sink-connector

# The timeout used to detect worker failures. The worker sends periodic heartbeats
# to indicate its liveness to the broker
session.timeout.ms=10000

# If a Connect worker leaves the group, intentionally or due to a failure, Connect
# waits for scheduled.rebalance.max.delay.ms before triggering a rebalance
scheduled.rebalance.max.delay.ms=60000
					</programlisting>
				</programlistingco>
			</listitem>
			<listitem>
				<para>connect-distributed.sh</para>
				<programlistingco>
					<programlisting>
...
if [ "x$KAFKA_HEAP_OPTS" = "x" ]; then
  export KAFKA_HEAP_OPTS="-Xms10G -Xmx10G"
fi
...
					</programlisting>
				</programlistingco>
			</listitem>
		</orderedlist>
	</section>
</section>